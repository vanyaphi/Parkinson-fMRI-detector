{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkinson's Disease Detection from fMRI Data on AWS SageMaker\n",
    "\n",
    "This notebook implements a machine learning pipeline for detecting Parkinson's disease from functional MRI (fMRI) data.\n",
    "Based on neuroimaging research methodologies for movement disorder classification.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Classify Parkinson's disease patients vs. healthy controls using fMRI features\n",
    "- **Approach**: Extract functional connectivity and regional activity features\n",
    "- **Methods**: Support Vector Machine, Random Forest, and Deep Learning classifiers\n",
    "- **Validation**: Cross-validation with performance metrics\n",
    "\n",
    "## Prerequisites\n",
    "- AWS SageMaker notebook instance in us-east-2 region\n",
    "- fMRI data stored in S3 (preprocessed with fMRIPrep)\n",
    "- Required libraries: nilearn, scikit-learn, tensorflow, nibabel\n",
    "\n",
    "## Dataset Structure Expected\n",
    "```\n",
    "s3://bucket/datasets/\n",
    "â”œâ”€â”€ controls/\n",
    "â”‚   â”œâ”€â”€ sub-001_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "â”‚   â””â”€â”€ sub-002_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "â””â”€â”€ patients/\n",
    "    â”œâ”€â”€ sub-101_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "    â””â”€â”€ sub-102_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Parkinson's detection\n",
    "!pip install nilearn scikit-learn tensorflow pandas numpy matplotlib seaborn\n",
    "!pip install nibabel boto3 sagemaker plotly imbalanced-learn\n",
    "!pip install xgboost lightgbm optuna scipy statsmodels\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sklearn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fMRI and neuroimaging imports\n",
    "import nibabel as nib\n",
    "from nilearn import datasets, plotting, image\n",
    "from nilearn.input_data import NiftiLabelsMasker, NiftiMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.plotting import plot_connectome, plot_matrix\n",
    "from nilearn.glm.first_level import FirstLevelModel, make_first_level_design_matrix\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AWS SageMaker Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS region and SageMaker session\n",
    "region = 'us-east-2'\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "# Get SageMaker execution role\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(f\"SageMaker execution role: {role}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get execution role: {e}\")\n",
    "    # For notebook instances, we can get the role from instance metadata\n",
    "    import urllib.request\n",
    "    import json\n",
    "    try:\n",
    "        # Get instance profile\n",
    "        response = urllib.request.urlopen('http://169.254.169.254/latest/meta-data/iam/security-credentials/')\n",
    "        role_name = response.read().decode('utf-8')\n",
    "        role = f\"arn:aws:iam::{boto3.client('sts').get_caller_identity()['Account']}:role/{role_name}\"\n",
    "        print(f\"Using role from instance metadata: {role}\")\n",
    "    except:\n",
    "        role = None\n",
    "        print(\"Could not determine execution role\")\n",
    "\n",
    "# Set up S3 bucket for data storage\n",
    "try:\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "except:\n",
    "    # If default bucket fails, try to find existing buckets\n",
    "    s3_client = boto3.client('s3', region_name=region)\n",
    "    buckets = s3_client.list_buckets()['Buckets']\n",
    "    fmri_buckets = [b['Name'] for b in buckets if 'fmri' in b['Name'].lower()]\n",
    "    if fmri_buckets:\n",
    "        bucket = fmri_buckets[0]\n",
    "        print(f\"Using existing fMRI bucket: {bucket}\")\n",
    "    else:\n",
    "        bucket = f\"sagemaker-{region}-{boto3.client('sts').get_caller_identity()['Account']}\"\n",
    "        print(f\"Using default bucket pattern: {bucket}\")\n",
    "\n",
    "prefix = 'parkinson-fmri-detection'\n",
    "\n",
    "print(f\"Using S3 bucket: {bucket}\")\n",
    "print(f\"Data prefix: {prefix}\")\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = '/home/ec2-user/SageMaker/data'  # SageMaker notebook instance path\n",
    "output_dir = '/home/ec2-user/SageMaker/output'\n",
    "models_dir = '/home/ec2-user/SageMaker/models'\n",
    "\n",
    "# Create directories\n",
    "for directory in [data_dir, output_dir, models_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fmri_dataset(s3_bucket, s3_prefix='datasets'):\n",
    "    \"\"\"\n",
    "    Load fMRI dataset from S3 with labels (controls vs patients)\n",
    "    \n",
    "    Expected S3 structure:\n",
    "    s3://bucket/datasets/controls/sub-*.nii.gz\n",
    "    s3://bucket/datasets/patients/sub-*.nii.gz\n",
    "    \n",
    "    Returns:\n",
    "    file_paths: List of local file paths\n",
    "    labels: List of labels (0=control, 1=patient)\n",
    "    subject_ids: List of subject identifiers\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    subject_ids = []\n",
    "    \n",
    "    # Download controls data\n",
    "    try:\n",
    "        controls_objects = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket, \n",
    "            Prefix=f'{s3_prefix}/controls/'\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in controls_objects:\n",
    "            for obj in controls_objects['Contents']:\n",
    "                if obj['Key'].endswith('.nii.gz'):\n",
    "                    local_path = os.path.join(data_dir, 'controls', os.path.basename(obj['Key']))\n",
    "                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "                    \n",
    "                    s3_client.download_file(s3_bucket, obj['Key'], local_path)\n",
    "                    file_paths.append(local_path)\n",
    "                    labels.append(0)  # Control\n",
    "                    \n",
    "                    # Extract subject ID\n",
    "                    filename = os.path.basename(obj['Key'])\n",
    "                    subject_id = filename.split('_')[0].replace('sub-', '')\n",
    "                    subject_ids.append(f'control_{subject_id}')\n",
    "                    \n",
    "            print(f\"Downloaded {len([l for l in labels if l == 0])} control subjects\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading controls: {e}\")\n",
    "    \n",
    "    # Download patients data\n",
    "    try:\n",
    "        patients_objects = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket, \n",
    "            Prefix=f'{s3_prefix}/patients/'\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in patients_objects:\n",
    "            for obj in patients_objects['Contents']:\n",
    "                if obj['Key'].endswith('.nii.gz'):\n",
    "                    local_path = os.path.join(data_dir, 'patients', os.path.basename(obj['Key']))\n",
    "                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "                    \n",
    "                    s3_client.download_file(s3_bucket, obj['Key'], local_path)\n",
    "                    file_paths.append(local_path)\n",
    "                    labels.append(1)  # Patient\n",
    "                    \n",
    "                    # Extract subject ID\n",
    "                    filename = os.path.basename(obj['Key'])\n",
    "                    subject_id = filename.split('_')[0].replace('sub-', '')\n",
    "                    subject_ids.append(f'patient_{subject_id}')\n",
    "                    \n",
    "            print(f\"Downloaded {len([l for l in labels if l == 1])} patient subjects\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading patients: {e}\")\n",
    "    \n",
    "    # If no real data available, create sample data for demonstration\n",
    "    if len(file_paths) == 0:\n",
    "        print(\"No data found in S3. Creating sample dataset for demonstration...\")\n",
    "        \n",
    "        try:\n",
    "            # Load sample motor task data from nilearn\n",
    "            motor_images = datasets.fetch_neurovault_motor_task()\n",
    "            \n",
    "            # Use first few images as controls and patients\n",
    "            for i, img_path in enumerate(motor_images.images[:10]):\n",
    "                local_path = os.path.join(data_dir, f'sample_sub-{i:03d}.nii.gz')\n",
    "                # Copy the file\n",
    "                import shutil\n",
    "                shutil.copy(img_path, local_path)\n",
    "                \n",
    "                file_paths.append(local_path)\n",
    "                labels.append(i % 2)  # Alternate between control (0) and patient (1)\n",
    "                subject_ids.append(f'sample_{i:03d}')\n",
    "            \n",
    "            print(f\"Created sample dataset with {len(file_paths)} subjects\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating sample dataset: {e}\")\n",
    "            # Create minimal dummy data\n",
    "            print(\"Creating minimal dummy dataset...\")\n",
    "            for i in range(4):\n",
    "                # Create dummy NIfTI files\n",
    "                dummy_data = np.random.randn(64, 64, 30, 100)  # Small 4D image\n",
    "                dummy_img = nib.Nifti1Image(dummy_data, affine=np.eye(4))\n",
    "                local_path = os.path.join(data_dir, f'dummy_sub-{i:03d}.nii.gz')\n",
    "                dummy_img.to_filename(local_path)\n",
    "                \n",
    "                file_paths.append(local_path)\n",
    "                labels.append(i % 2)\n",
    "                subject_ids.append(f'dummy_{i:03d}')\n",
    "            \n",
    "            print(f\"Created dummy dataset with {len(file_paths)} subjects\")\n",
    "    \n",
    "    return file_paths, labels, subject_ids\n",
    "\n",
    "# Load the dataset\n",
    "file_paths, labels, subject_ids = load_fmri_dataset(bucket, prefix)\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Total subjects: {len(file_paths)}\")\n",
    "print(f\"Controls: {labels.count(0)}\")\n",
    "print(f\"Patients: {labels.count(1)}\")\n",
    "\n",
    "# Create DataFrame for easier handling\n",
    "dataset_df = pd.DataFrame({\n",
    "    'subject_id': subject_ids,\n",
    "    'file_path': file_paths,\n",
    "    'label': labels,\n",
    "    'group': ['Control' if l == 0 else 'Patient' for l in labels]\n",
    "})\n",
    "\n",
    "print(\"\\nDataset DataFrame:\")\n",
    "print(dataset_df.head())\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(dataset_df['group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction from fMRI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brain atlas for ROI extraction\n",
    "print(\"Loading brain atlas...\")\n",
    "\n",
    "try:\n",
    "    # Use Harvard-Oxford atlas for ROI definition\n",
    "    atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "    atlas_labels = atlas.labels[1:]  # Remove background label\n",
    "    \n",
    "    print(f\"Atlas loaded with {len(atlas_labels)} regions\")\n",
    "    print(f\"First 10 regions: {atlas_labels[:10]}\")\n",
    "    \n",
    "    # Initialize masker for ROI extraction\n",
    "    masker = NiftiLabelsMasker(\n",
    "        labels_img=atlas.maps,\n",
    "        standardize=True,\n",
    "        memory='nilearn_cache',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"ROI masker initialized\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading atlas: {e}\")\n",
    "    print(\"Using simplified approach with whole-brain masker...\")\n",
    "    \n",
    "    # Fallback to whole-brain masker\n",
    "    masker = NiftiMasker(\n",
    "        standardize=True,\n",
    "        memory='nilearn_cache',\n",
    "        verbose=0\n",
    "    )\n",
    "    atlas_labels = [f'Region_{i}' for i in range(100)]  # Dummy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_fmri(file_path, masker):\n",
    "    \"\"\"\n",
    "    Extract features from a single fMRI file\n",
    "    \n",
    "    Features extracted:\n",
    "    1. ROI time series mean and std\n",
    "    2. Functional connectivity matrix (correlation)\n",
    "    3. Regional homogeneity measures\n",
    "    \n",
    "    Returns:\n",
    "    features: 1D numpy array of features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load fMRI image\n",
    "        img = nib.load(file_path)\n",
    "        \n",
    "        # Extract time series from ROIs\n",
    "        time_series = masker.fit_transform(img)\n",
    "        \n",
    "        # Ensure we have a reasonable number of features\n",
    "        if time_series.shape[1] > 1000:\n",
    "            # Reduce dimensionality if too many voxels\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA(n_components=100)\n",
    "            time_series = pca.fit_transform(time_series.T).T\n",
    "        \n",
    "        # Feature 1: Statistical measures of time series\n",
    "        roi_means = np.mean(time_series, axis=0)\n",
    "        roi_stds = np.std(time_series, axis=0)\n",
    "        roi_vars = np.var(time_series, axis=0)\n",
    "        \n",
    "        # Feature 2: Functional connectivity (correlation matrix)\n",
    "        connectivity_measure = ConnectivityMeasure(kind='correlation')\n",
    "        correlation_matrix = connectivity_measure.fit_transform([time_series])[0]\n",
    "        \n",
    "        # Extract upper triangle of correlation matrix (avoid redundancy)\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "        connectivity_features = correlation_matrix[mask]\n",
    "        \n",
    "        # Feature 3: Network measures\n",
    "        # Mean connectivity strength per ROI\n",
    "        connectivity_strength = np.mean(np.abs(correlation_matrix), axis=1)\n",
    "        \n",
    "        # Feature 4: Simple frequency domain features\n",
    "        freq_features = []\n",
    "        for roi_ts in time_series.T[:min(10, time_series.shape[1])]:\n",
    "            # Simple power measures\n",
    "            fft_vals = np.fft.fft(roi_ts)\n",
    "            power_spectrum = np.abs(fft_vals)**2\n",
    "            \n",
    "            # Extract power in different frequency bands\n",
    "            low_freq_power = np.mean(power_spectrum[:len(power_spectrum)//4])\n",
    "            high_freq_power = np.mean(power_spectrum[len(power_spectrum)//4:])\n",
    "            \n",
    "            freq_features.extend([low_freq_power, high_freq_power])\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = np.concatenate([\n",
    "            roi_means,\n",
    "            roi_stds,\n",
    "            roi_vars,\n",
    "            connectivity_features,\n",
    "            connectivity_strength,\n",
    "            freq_features\n",
    "        ])\n",
    "        \n",
    "        # Handle any NaN or infinite values\n",
    "        all_features = np.nan_to_num(all_features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        return all_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract features from all subjects\n",
    "print(\"Extracting features from all subjects...\")\n",
    "features_list = []\n",
    "valid_labels = []\n",
    "valid_subjects = []\n",
    "\n",
    "for i, (file_path, label, subject_id) in enumerate(zip(file_paths, labels, subject_ids)):\n",
    "    print(f\"Processing {i+1}/{len(file_paths)}: {subject_id}\")\n",
    "    \n",
    "    features = extract_features_from_fmri(file_path, masker)\n",
    "    \n",
    "    if features is not None:\n",
    "        features_list.append(features)\n",
    "        valid_labels.append(label)\n",
    "        valid_subjects.append(subject_id)\n",
    "    else:\n",
    "        print(f\"Skipping {subject_id} due to processing error\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "if len(features_list) > 0:\n",
    "    X = np.array(features_list)\n",
    "    y = np.array(valid_labels)\n",
    "    \n",
    "    print(f\"\\nFeature extraction completed!\")\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Number of features per subject: {X.shape[1]}\")\n",
    "    print(f\"Valid subjects: {len(valid_subjects)}\")\n",
    "    print(f\"Controls: {np.sum(y == 0)}, Patients: {np.sum(y == 1)}\")\n",
    "else:\n",
    "    print(\"No features extracted. Please check your data.\")\n",
    "    X, y = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None and len(X) > 1:\n",
    "    # Prepare data for machine learning\n",
    "    print(\"Preparing data for machine learning...\")\n",
    "    \n",
    "    # Handle missing values (if any)\n",
    "    X_clean = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Feature selection - keep top features by variance\n",
    "    feature_vars = np.var(X_clean, axis=0)\n",
    "    top_features_idx = np.argsort(feature_vars)[-min(100, X_clean.shape[1]):]\n",
    "    X_selected = X_clean[:, top_features_idx]\n",
    "    \n",
    "    print(f\"Selected feature matrix shape: {X_selected.shape}\")\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    if len(np.unique(y)) > 1 and len(X_selected) > 2:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_selected, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "        print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "        print(f\"Training set distribution: {np.bincount(y_train)}\")\n",
    "        print(f\"Test set distribution: {np.bincount(y_test)}\")\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        print(\"Data preprocessing completed\")\n",
    "        \n",
    "        # Define and train classifiers\n",
    "        classifiers = {\n",
    "            'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        # Train and evaluate classifiers\n",
    "        results = {}\n",
    "        trained_models = {}\n",
    "        \n",
    "        print(\"Training and evaluating classifiers...\")\n",
    "        \n",
    "        for name, clf in classifiers.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Train the classifier\n",
    "                clf.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = clf.predict(X_test_scaled)\n",
    "                y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = clf.score(X_test_scaled, y_test)\n",
    "                auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "                \n",
    "                # Cross-validation (if enough samples)\n",
    "                if len(X_train_scaled) >= 5:\n",
    "                    cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=min(3, len(X_train_scaled)), scoring='accuracy')\n",
    "                    cv_mean = cv_scores.mean()\n",
    "                    cv_std = cv_scores.std()\n",
    "                else:\n",
    "                    cv_mean = accuracy\n",
    "                    cv_std = 0.0\n",
    "                \n",
    "                results[name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'auc': auc_score,\n",
    "                    'cv_mean': cv_mean,\n",
    "                    'cv_std': cv_std,\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_pred_proba\n",
    "                }\n",
    "                \n",
    "                trained_models[name] = clf\n",
    "                \n",
    "                print(f\"{name} Results:\")\n",
    "                print(f\"  Test Accuracy: {accuracy:.3f}\")\n",
    "                print(f\"  AUC Score: {auc_score:.3f}\")\n",
    "                print(f\"  CV Accuracy: {cv_mean:.3f} Â± {cv_std:.3f}\")\n",
    "                \n",
    "                # Classification report\n",
    "                print(f\"\\nClassification Report for {name}:\")\n",
    "                print(classification_report(y_test, y_pred, target_names=['Control', 'Patient']))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name}: {e}\")\n",
    "        \n",
    "        print(\"\\nClassifier training completed!\")\n",
    "        \n",
    "        # Plot ROC curves if we have results\n",
    "        if results:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            colors = ['blue', 'red', 'green', 'orange']\n",
    "            \n",
    "            for i, (model_name, result) in enumerate(results.items()):\n",
    "                fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "                auc_score = result['auc']\n",
    "                \n",
    "                plt.plot(fpr, tpr, color=colors[i % len(colors)], lw=2,\n",
    "                         label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "            \n",
    "            # Plot diagonal line\n",
    "            plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.8)\n",
    "            \n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC Curves - Parkinson\\'s Disease Detection')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Save ROC plot\n",
    "            plt.savefig(os.path.join(output_dir, 'roc_curves.png'), dpi=300, bbox_inches='tight')\n",
    "            print(\"ROC curves plot saved\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Not enough samples or classes for train/test split\")\n",
    "else:\n",
    "    print(\"No valid features extracted for machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "from datetime import datetime\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "# Parkinson's Disease fMRI Detection - Analysis Summary Report\n",
    "\n",
    "## Analysis Details\n",
    "- **Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **AWS Region**: {region}\n",
    "- **S3 Bucket**: {bucket}\n",
    "- **Data Prefix**: {prefix}\n",
    "\n",
    "## Dataset Information\n",
    "- **Total Subjects**: {len(valid_subjects) if 'valid_subjects' in locals() else 'N/A'}\n",
    "- **Controls**: {np.sum(y == 0) if 'y' in locals() and y is not None else 'N/A'}\n",
    "- **Patients**: {np.sum(y == 1) if 'y' in locals() and y is not None else 'N/A'}\n",
    "- **Features Extracted**: {X.shape[1] if 'X' in locals() and X is not None else 'N/A'}\n",
    "\n",
    "## Model Performance Summary\n",
    "\"\"\"\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    for model_name, result in results.items():\n",
    "        summary_report += f\"\"\"\n",
    "### {model_name}\n",
    "- **Test Accuracy**: {result['accuracy']:.3f}\n",
    "- **AUC Score**: {result['auc']:.3f}\n",
    "- **Cross-Validation**: {result['cv_mean']:.3f} Â± {result['cv_std']:.3f}\n",
    "\"\"\"\n",
    "else:\n",
    "    summary_report += \"\\nNo model results available.\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "## Generated Files\n",
    "- ROC curves: roc_curves.png\n",
    "- Analysis summary: analysis_summary.md\n",
    "\n",
    "## S3 Locations\n",
    "- **Results**: s3://{bucket}/{prefix}/results/\n",
    "\n",
    "## Notes\n",
    "- This analysis demonstrates the fMRI-based Parkinson's detection pipeline\n",
    "- Results may vary based on dataset quality and size\n",
    "- For production use, ensure adequate sample sizes and proper validation\n",
    "\n",
    "---\n",
    "*Report generated automatically by Parkinson's fMRI Detection Pipeline*\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "report_path = os.path.join(output_dir, 'analysis_summary.md')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARKINSON'S DISEASE fMRI DETECTION ANALYSIS COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(summary_report)\n",
    "\n",
    "# Upload results to S3\n",
    "def upload_results_to_s3(local_dir, s3_bucket, s3_prefix):\n",
    "    \"\"\"\n",
    "    Upload analysis results to S3\n",
    "    \"\"\"\n",
    "    uploaded_files = []\n",
    "    \n",
    "    try:\n",
    "        for root, dirs, files in os.walk(local_dir):\n",
    "            for file in files:\n",
    "                local_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(local_path, local_dir)\n",
    "                s3_key = f\"{s3_prefix}/results/{relative_path}\"\n",
    "                \n",
    "                try:\n",
    "                    s3_client.upload_file(local_path, s3_bucket, s3_key)\n",
    "                    uploaded_files.append(f\"s3://{s3_bucket}/{s3_key}\")\n",
    "                    print(f\"Uploaded {relative_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading {local_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing directory {local_dir}: {e}\")\n",
    "    \n",
    "    return uploaded_files\n",
    "\n",
    "# Upload all results\n",
    "print(\"\\nUploading results to S3...\")\n",
    "uploaded_files = upload_results_to_s3(output_dir, bucket, prefix)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Analysis completed successfully!\")\n",
    "print(f\"ðŸ“Š Results uploaded to: s3://{bucket}/{prefix}/results/\")\n",
    "print(f\"ðŸ§  Ready for further analysis and validation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
