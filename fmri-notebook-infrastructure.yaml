AWSTemplateFormatVersion: '2010-09-09'
Description: 'CloudFormation template for Parkinson fMRI detection using SageMaker Notebook Instance with GitHub integration'

Parameters:
  NotebookInstanceName:
    Type: String
    Default: 'parkinson-fmri-detector'
    Description: 'Name for the SageMaker notebook instance'
    
  InstanceType:
    Type: String
    Default: 'ml.t3.medium'
    AllowedValues:
      - ml.t3.medium
      - ml.t3.large
      - ml.t3.xlarge
      - ml.m5.large
      - ml.m5.xlarge
      - ml.m5.2xlarge
    Description: 'SageMaker notebook instance type'
    
  BucketName:
    Type: String
    Default: 'fmri-dataset-bucket'
    Description: 'Name for the S3 bucket to store fMRI datasets (must be globally unique)'
    
  GitHubRepository:
    Type: String
    Default: 'https://github.com/vanyaphi/Parkinson-fMRI-detector.git'
    Description: 'GitHub repository URL to clone into the notebook instance'
    
  VolumeSize:
    Type: Number
    Default: 20
    MinValue: 5
    MaxValue: 16384
    Description: 'Size of the EBS volume in GB for the notebook instance'

Conditions:
  HasCustomGitRepo: !Not [!Equals [!Ref GitHubRepository, '']]

Resources:
  # S3 Bucket for fMRI datasets
  FMRIDatasetBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${BucketName}-${AWS::AccountId}-${AWS::Region}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: DeleteIncompleteMultipartUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
          - Id: TransitionToIA
            Status: Enabled
            Transitions:
              - TransitionInDays: 30
                StorageClass: STANDARD_IA
              - TransitionInDays: 90
                StorageClass: GLACIER
              - TransitionInDays: 365
                StorageClass: DEEP_ARCHIVE
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpiration:
              NoncurrentDays: 90
      NotificationConfiguration:
        CloudWatchConfigurations:
          - Event: s3:ObjectCreated:*
            CloudWatchConfiguration:
              LogGroupName: !Ref S3AccessLogGroup
      Tags:
        - Key: Purpose
          Value: 'fMRI Dataset Storage'
        - Key: Project
          Value: 'Parkinson Detection'
        - Key: CostCenter
          Value: 'Research'

  # CloudWatch Log Group for S3 access logs
  S3AccessLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/s3/${BucketName}-access-logs'
      RetentionInDays: 30

  # S3 Bucket Policy for secure access
  FMRIDatasetBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref FMRIDatasetBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: DenyInsecureConnections
            Effect: Deny
            Principal: '*'
            Action: 's3:*'
            Resource:
              - !Sub '${FMRIDatasetBucket}/*'
              - !GetAtt FMRIDatasetBucket.Arn
            Condition:
              Bool:
                'aws:SecureTransport': 'false'
          - Sid: AllowSageMakerAccess
            Effect: Allow
            Principal:
              AWS: !GetAtt SageMakerNotebookRole.Arn
            Action:
              - 's3:GetObject'
              - 's3:PutObject'
              - 's3:DeleteObject'
              - 's3:ListBucket'
              - 's3:GetBucketLocation'
            Resource:
              - !Sub '${FMRIDatasetBucket}/*'
              - !GetAtt FMRIDatasetBucket.Arn

  # IAM Role for SageMaker Notebook Instance
  SageMakerNotebookRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${NotebookInstanceName}-execution-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - sagemaker.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
      Policies:
        - PolicyName: S3AccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:ListBucket
                  - s3:GetBucketLocation
                  - s3:GetBucketVersioning
                Resource:
                  - !Sub '${FMRIDatasetBucket}/*'
                  - !GetAtt FMRIDatasetBucket.Arn
              - Effect: Allow
                Action:
                  - s3:ListAllMyBuckets
                  - s3:GetBucketLocation
                Resource: '*'
        - PolicyName: CloudWatchLogsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - logs:DescribeLogStreams
                  - logs:DescribeLogGroups
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*'
        - PolicyName: ECRAccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ecr:GetAuthorizationToken
                  - ecr:BatchCheckLayerAvailability
                  - ecr:GetDownloadUrlForLayer
                  - ecr:BatchGetImage
                Resource: '*'
        - PolicyName: SageMakerNotebookPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - sagemaker:CreateTrainingJob
                  - sagemaker:CreateProcessingJob
                  - sagemaker:CreateModel
                  - sagemaker:CreateEndpoint
                  - sagemaker:CreateEndpointConfig
                  - sagemaker:DescribeTrainingJob
                  - sagemaker:DescribeProcessingJob
                  - sagemaker:DescribeModel
                  - sagemaker:DescribeEndpoint
                  - sagemaker:DescribeEndpointConfig
                  - sagemaker:DescribeNotebookInstance
                  - sagemaker:StopNotebookInstance
                  - sagemaker:ListTrainingJobs
                  - sagemaker:ListProcessingJobs
                  - sagemaker:ListModels
                  - sagemaker:ListEndpoints
                  - sagemaker:StopTrainingJob
                  - sagemaker:StopProcessingJob
                  - sagemaker:DeleteEndpoint
                  - sagemaker:DeleteEndpointConfig
                  - sagemaker:DeleteModel
                Resource: '*'

  # Lifecycle Configuration for auto-shutdown and setup
  NotebookLifecycleConfig:
    Type: AWS::SageMaker::NotebookInstanceLifecycleConfig
    Properties:
      NotebookInstanceLifecycleConfigName: !Sub '${NotebookInstanceName}-lifecycle'
      OnCreate:
        - Content: !Base64 |
            #!/bin/bash
            set -e
            
            # Update system packages
            sudo yum update -y
            
            # Install additional Python packages for fMRI analysis
            sudo -u ec2-user -i <<'EOF'
            source activate python3
            
            # Install neuroimaging packages
            pip install --upgrade pip
            pip install nilearn nibabel
            pip install scikit-learn tensorflow pandas numpy matplotlib seaborn
            pip install plotly imbalanced-learn
            pip install xgboost lightgbm optuna
            pip install boto3 sagemaker
            
            # Install additional scientific packages
            pip install scipy statsmodels
            pip install jupyter-contrib-nbextensions
            
            echo "Package installation completed"
            EOF
            
            # Create auto-shutdown script
            cat > /home/ec2-user/SageMaker/auto-shutdown.py << 'SCRIPT'
            #!/usr/bin/env python3
            import boto3
            import json
            import os
            import time
            import subprocess
            from datetime import datetime, timedelta
            import psutil
            
            IDLE_TIME_MINUTES = 30
            CHECK_INTERVAL_SECONDS = 300  # 5 minutes
            
            def is_notebook_idle():
                """Check if notebook is idle based on kernel activity and system usage"""
                try:
                    # Check if any Jupyter kernels are running
                    result = subprocess.run(['pgrep', '-f', 'ipykernel'], 
                                          capture_output=True, text=True)
                    if result.returncode == 0:
                        # Kernels are running, check CPU usage
                        cpu_percent = psutil.cpu_percent(interval=1)
                        if cpu_percent > 10.0:  # More than 10% CPU usage
                            return False
                    
                    # Check for recent file modifications in SageMaker directory
                    sagemaker_dir = '/home/ec2-user/SageMaker'
                    cutoff_time = time.time() - (IDLE_TIME_MINUTES * 60)
                    
                    for root, dirs, files in os.walk(sagemaker_dir):
                        for file in files:
                            if file.endswith(('.ipynb', '.py', '.R')):
                                file_path = os.path.join(root, file)
                                if os.path.getmtime(file_path) > cutoff_time:
                                    return False
                    
                    return True
                    
                except Exception as e:
                    print(f"Error checking idle status: {e}")
                    return False  # Assume not idle if we can't check
            
            def shutdown_instance():
                """Shutdown the notebook instance"""
                try:
                    # Get instance metadata
                    response = subprocess.run(['curl', '-s', 
                                             'http://169.254.169.254/latest/meta-data/instance-id'],
                                           capture_output=True, text=True)
                    instance_id = response.stdout.strip()
                    
                    if not instance_id:
                        print("Could not get instance ID")
                        return
                    
                    # Get region
                    response = subprocess.run(['curl', '-s',
                                             'http://169.254.169.254/latest/meta-data/placement/region'],
                                           capture_output=True, text=True)
                    region = response.stdout.strip()
                    
                    if not region:
                        region = 'us-east-2'  # Default region
                    
                    # Stop the notebook instance
                    client = boto3.client('sagemaker', region_name=region)
                    
                    # Get notebook instance name from tags or metadata
                    ec2_client = boto3.client('ec2', region_name=region)
                    instance_info = ec2_client.describe_instances(InstanceIds=[instance_id])
                    
                    notebook_name = None
                    for reservation in instance_info['Reservations']:
                        for instance in reservation['Instances']:
                            for tag in instance.get('Tags', []):
                                if tag['Key'] == 'aws:sagemaker:notebook-instance-name':
                                    notebook_name = tag['Value']
                                    break
                    
                    if notebook_name:
                        print(f"Stopping notebook instance: {notebook_name}")
                        client.stop_notebook_instance(NotebookInstanceName=notebook_name)
                        print("Shutdown initiated successfully")
                    else:
                        print("Could not determine notebook instance name")
                        
                except Exception as e:
                    print(f"Error during shutdown: {e}")
            
            def main():
                idle_start_time = None
                
                while True:
                    if is_notebook_idle():
                        if idle_start_time is None:
                            idle_start_time = time.time()
                            print(f"Idle period started at {datetime.now()}")
                        elif time.time() - idle_start_time > (IDLE_TIME_MINUTES * 60):
                            print(f"Instance idle for {IDLE_TIME_MINUTES} minutes. Shutting down...")
                            shutdown_instance()
                            break
                    else:
                        if idle_start_time is not None:
                            print(f"Activity resumed at {datetime.now()}")
                        idle_start_time = None
                    
                    time.sleep(CHECK_INTERVAL_SECONDS)
            
            if __name__ == "__main__":
                main()
            SCRIPT
            
            chmod +x /home/ec2-user/SageMaker/auto-shutdown.py
            
            # Create systemd service for auto-shutdown
            cat > /etc/systemd/system/sagemaker-auto-shutdown.service << 'SERVICE'
            [Unit]
            Description=SageMaker Notebook Auto-shutdown
            After=network.target
            
            [Service]
            Type=simple
            User=ec2-user
            WorkingDirectory=/home/ec2-user/SageMaker
            ExecStart=/usr/bin/python3 /home/ec2-user/SageMaker/auto-shutdown.py
            Restart=always
            RestartSec=60
            
            [Install]
            WantedBy=multi-user.target
            SERVICE
            
            # Enable and start the auto-shutdown service
            systemctl daemon-reload
            systemctl enable sagemaker-auto-shutdown.service
            systemctl start sagemaker-auto-shutdown.service
            
            echo "Auto-shutdown service configured and started"
            
      OnStart:
        - Content: !Base64 |
            #!/bin/bash
            set -e
            
            # Restart auto-shutdown service on notebook start
            systemctl restart sagemaker-auto-shutdown.service
            
            # Update environment variables
            sudo -u ec2-user -i <<'EOF'
            echo "export AWS_DEFAULT_REGION=$(curl -s http://169.254.169.254/latest/meta-data/placement/region)" >> ~/.bashrc
            echo "Notebook instance started successfully"
            EOF

  # SageMaker Notebook Instance
  SageMakerNotebookInstance:
    Type: AWS::SageMaker::NotebookInstance
    Properties:
      NotebookInstanceName: !Ref NotebookInstanceName
      InstanceType: !Ref InstanceType
      RoleArn: !GetAtt SageMakerNotebookRole.Arn
      VolumeSizeInGB: !Ref VolumeSize
      DefaultCodeRepository: !If
        - HasCustomGitRepo
        - !Ref GitHubRepository
        - !Ref AWS::NoValue
      LifecycleConfigName: !Ref NotebookLifecycleConfig
      Tags:
        - Key: Purpose
          Value: 'Parkinson fMRI Detection'
        - Key: Project
          Value: 'Neuroimaging Research'
        - Key: AutoShutdown
          Value: 'Enabled'

  # Lambda function to upload sample notebook
  NotebookUploadFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${NotebookInstanceName}-notebook-uploader'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt NotebookUploadRole.Arn
      Timeout: 300
      Code:
        ZipFile: |
          import boto3
          import json
          import cfnresponse
          
          def lambda_handler(event, context):
              try:
                  if event['RequestType'] == 'Create' or event['RequestType'] == 'Update':
                      s3_client = boto3.client('s3')
                      bucket_name = event['ResourceProperties']['BucketName']
                      
                      # Create sample folder structure
                      folders = [
                          'datasets/controls/',
                          'datasets/patients/',
                          'preprocessed/',
                          'results/',
                          'models/'
                      ]
                      
                      for folder in folders:
                          s3_client.put_object(
                              Bucket=bucket_name,
                              Key=f'{folder}README.txt',
                              Body=f'Upload your data to the {folder} folder',
                              ContentType='text/plain'
                          )
                      
                      # Create a sample dataset info file
                      dataset_info = """# Parkinson's fMRI Dataset Structure"""
                      
                      s3_client.put_object(
                          Bucket=bucket_name,
                          Key='DATASET_INFO.md',
                          Body=dataset_info,
                          ContentType='text/markdown'
                      )
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                          'DatasetS3Location': f's3://{bucket_name}/'
                      })
                  else:
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              except Exception as e:
                  print(f"Error: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {})

  # IAM Role for Lambda function
  NotebookUploadRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3UploadPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:PutObjectAcl
                Resource: !Sub '${FMRIDatasetBucket}/*'

  # Custom resource to setup S3 structure
  S3Setup:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt NotebookUploadFunction.Arn
      BucketName: !Ref FMRIDatasetBucket

Outputs:
  NotebookInstanceName:
    Description: 'SageMaker Notebook Instance Name'
    Value: !Ref SageMakerNotebookInstance
    Export:
      Name: !Sub '${AWS::StackName}-NotebookInstance'

  NotebookInstanceUrl:
    Description: 'SageMaker Notebook Instance URL'
    Value: !Sub 'https://${SageMakerNotebookInstance}.notebook.${AWS::Region}.sagemaker.aws/tree'
    Export:
      Name: !Sub '${AWS::StackName}-NotebookUrl'

  S3BucketName:
    Description: 'S3 Bucket for fMRI datasets'
    Value: !Ref FMRIDatasetBucket
    Export:
      Name: !Sub '${AWS::StackName}-S3Bucket'

  S3BucketArn:
    Description: 'S3 Bucket ARN'
    Value: !GetAtt FMRIDatasetBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-S3BucketArn'

  ExecutionRoleArn:
    Description: 'SageMaker Execution Role ARN'
    Value: !GetAtt SageMakerNotebookRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-ExecutionRole'

  GitHubRepository:
    Description: 'GitHub repository cloned to notebook'
    Value: !Ref GitHubRepository
    Export:
      Name: !Sub '${AWS::StackName}-GitHubRepo'

  DatasetS3Location:
    Description: 'S3 location for datasets'
    Value: !GetAtt S3Setup.DatasetS3Location
    Export:
      Name: !Sub '${AWS::StackName}-DatasetLocation'