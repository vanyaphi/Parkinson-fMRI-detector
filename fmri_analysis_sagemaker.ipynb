{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkinson's Disease Detection from fMRI Data on AWS SageMaker\n",
    "\n",
    "This notebook implements a machine learning pipeline for detecting Parkinson's disease from functional MRI (fMRI) data.\n",
    "Based on neuroimaging research methodologies for movement disorder classification.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Classify Parkinson's disease patients vs. healthy controls using fMRI features\n",
    "- **Approach**: Extract functional connectivity and regional activity features\n",
    "- **Methods**: Support Vector Machine, Random Forest, and Deep Learning classifiers\n",
    "- **Validation**: Cross-validation with performance metrics\n",
    "\n",
    "## Prerequisites\n",
    "- AWS SageMaker domain in us-east-2 region\n",
    "- fMRI data stored in S3 (preprocessed with fMRIPrep)\n",
    "- Required libraries: nilearn, scikit-learn, tensorflow, nibabel\n",
    "\n",
    "## Dataset Structure Expected\n",
    "```\n",
    "s3://bucket/datasets/\n",
    "├── controls/\n",
    "│   ├── sub-001_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "│   └── sub-002_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "└── patients/\n",
    "    ├── sub-101_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "    └── sub-102_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Parkinson's detection\n",
    "!pip install nilearn scikit-learn tensorflow pandas numpy matplotlib seaborn\n",
    "!pip install nibabel boto3 sagemaker plotly imbalanced-learn\n",
    "!pip install xgboost lightgbm optuna  # For advanced ML models and hyperparameter tuning\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fMRI and neuroimaging imports\n",
    "import nibabel as nib\n",
    "from nilearn import datasets, plotting, image, connectome\n",
    "from nilearn.input_data import NiftiLabelsMasker, NiftiMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.plotting import plot_connectome, plot_matrix\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AWS SageMaker Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS region and SageMaker session\n",
    "region = 'us-east-2'\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "# Get SageMaker execution role\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"SageMaker execution role: {role}\")\n",
    "\n",
    "# Set up S3 bucket for data storage\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'fmri-analysis'\n",
    "\n",
    "print(f\"Using S3 bucket: {bucket}\")\n",
    "print(f\"Data prefix: {prefix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing with fmriprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths - update these paths to your actual data location\n",
    "# Assuming fmriprep preprocessed data is available in S3 or local storage\n",
    "\n",
    "# Example: Download sample data or use your preprocessed fmriprep output\n",
    "data_dir = '/opt/ml/processing/input'  # SageMaker processing input path\n",
    "output_dir = '/opt/ml/processing/output'  # SageMaker processing output path\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed fMRI data (assuming fmriprep output)\n",
    "# Update the file path to match your actual preprocessed data\n",
    "\n",
    "def load_fmri_data(subject_id, session='01', task='rest'):\n",
    "    \"\"\"\n",
    "    Load fmriprep preprocessed fMRI data\n",
    "    \n",
    "    Parameters:\n",
    "    subject_id (str): Subject identifier\n",
    "    session (str): Session identifier\n",
    "    task (str): Task name\n",
    "    \n",
    "    Returns:\n",
    "    img: Loaded NIfTI image\n",
    "    \"\"\"\n",
    "    # Example fmriprep output filename pattern\n",
    "    filename = f\"sub-{subject_id}_ses-{session}_task-{task}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        img = nib.load(filepath)\n",
    "        print(f\"Loaded fMRI data: {filename}\")\n",
    "        print(f\"Image shape: {img.shape}\")\n",
    "        return img\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        # For demonstration, load sample data\n",
    "        print(\"Loading sample dataset for demonstration...\")\n",
    "        motor_images = datasets.fetch_neurovault_motor_task()\n",
    "        return nib.load(motor_images.images[0])\n",
    "\n",
    "# Load data for a sample subject\n",
    "subject_id = '01'\n",
    "fmri_img = load_fmri_data(subject_id)\n",
    "\n",
    "# Display basic information about the loaded image\n",
    "print(f\"\\nImage dimensions: {fmri_img.shape}\")\n",
    "print(f\"Voxel size: {fmri_img.header.get_zooms()}\")\n",
    "print(f\"Data type: {fmri_img.get_fdata().dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis with nistat (nilearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experimental design matrix\n",
    "# This is a simplified example - adjust based on your experimental design\n",
    "\n",
    "def create_design_matrix(n_scans, tr=2.0):\n",
    "    \"\"\"\n",
    "    Create a simple design matrix for demonstration\n",
    "    \n",
    "    Parameters:\n",
    "    n_scans (int): Number of time points\n",
    "    tr (float): Repetition time in seconds\n",
    "    \n",
    "    Returns:\n",
    "    design_matrix: pandas DataFrame with design matrix\n",
    "    \"\"\"\n",
    "    from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "    \n",
    "    # Create frame times\n",
    "    frame_times = np.arange(n_scans) * tr\n",
    "    \n",
    "    # Simple block design example\n",
    "    conditions = ['rest', 'task']\n",
    "    onsets = [np.arange(0, n_scans*tr, 40), np.arange(20, n_scans*tr, 40)]\n",
    "    durations = [[20], [20]]\n",
    "    \n",
    "    events = pd.DataFrame({\n",
    "        'trial_type': np.repeat(conditions, [len(o) for o in onsets]),\n",
    "        'onset': np.concatenate(onsets),\n",
    "        'duration': np.concatenate(durations)\n",
    "    })\n",
    "    \n",
    "    design_matrix = make_first_level_design_matrix(\n",
    "        frame_times, events, hrf_model='glover'\n",
    "    )\n",
    "    \n",
    "    return design_matrix, events\n",
    "\n",
    "# Get number of time points from the fMRI data\n",
    "if len(fmri_img.shape) == 4:\n",
    "    n_scans = fmri_img.shape[3]\n",
    "else:\n",
    "    n_scans = 200  # Default for demonstration\n",
    "\n",
    "design_matrix, events = create_design_matrix(n_scans)\n",
    "\n",
    "print(f\"Design matrix shape: {design_matrix.shape}\")\n",
    "print(f\"Events shape: {events.shape}\")\n",
    "\n",
    "# Plot design matrix\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_design_matrix(design_matrix)\n",
    "plt.title('Experimental Design Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-level GLM analysis\n",
    "print(\"Running first-level GLM analysis...\")\n",
    "\n",
    "# Initialize first-level model\n",
    "first_level_model = FirstLevelModel(\n",
    "    t_r=2.0,\n",
    "    noise_model='ar1',\n",
    "    standardize=True,\n",
    "    hrf_model='glover',\n",
    "    drift_model='cosine',\n",
    "    high_pass=0.01\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "first_level_model = first_level_model.fit(fmri_img, design_matrices=design_matrix)\n",
    "\n",
    "# Compute contrasts\n",
    "contrast_matrix = np.eye(design_matrix.shape[1])\n",
    "contrasts = {\n",
    "    'task': contrast_matrix[0],  # Assuming first column is task condition\n",
    "    'rest': contrast_matrix[1] if design_matrix.shape[1] > 1 else contrast_matrix[0]\n",
    "}\n",
    "\n",
    "# Compute statistical maps\n",
    "stat_maps = {}\n",
    "for contrast_name, contrast_vector in contrasts.items():\n",
    "    try:\n",
    "        stat_map = first_level_model.compute_contrast(\n",
    "            contrast_vector, output_type='stat'\n",
    "        )\n",
    "        stat_maps[contrast_name] = stat_map\n",
    "        print(f\"Computed {contrast_name} contrast\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing {contrast_name} contrast: {e}\")\n",
    "\n",
    "print(\"First-level analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize statistical maps\n",
    "from nilearn import plotting\n",
    "\n",
    "# Plot statistical maps for each contrast\n",
    "for contrast_name, stat_map in stat_maps.items():\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot statistical map\n",
    "    plotting.plot_stat_map(\n",
    "        stat_map,\n",
    "        threshold=2.3,  # Adjust threshold as needed\n",
    "        display_mode='mosaic',\n",
    "        title=f'{contrast_name.capitalize()} Activation Map',\n",
    "        colorbar=True\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the statistical map\n",
    "    output_path = os.path.join(output_dir, f'stat_map_{contrast_name}.nii.gz')\n",
    "    stat_map.to_filename(output_path)\n",
    "    print(f\"Saved {contrast_name} statistical map to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time series from regions of interest (ROIs)\n",
    "from nilearn import input_data\n",
    "\n",
    "# Load atlas for ROI extraction\n",
    "atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "\n",
    "# Create masker for ROI extraction\n",
    "masker = input_data.NiftiLabelsMasker(\n",
    "    labels_img=atlas.maps,\n",
    "    standardize=True,\n",
    "    memory='nilearn_cache',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Extract time series\n",
    "time_series = masker.fit_transform(fmri_img)\n",
    "\n",
    "print(f\"Extracted time series shape: {time_series.shape}\")\n",
    "print(f\"Number of ROIs: {time_series.shape[1]}\")\n",
    "print(f\"Number of time points: {time_series.shape[0]}\")\n",
    "\n",
    "# Plot time series for first few ROIs\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(min(5, time_series.shape[1])):\n",
    "    plt.subplot(5, 1, i+1)\n",
    "    plt.plot(time_series[:, i])\n",
    "    plt.title(f'ROI {i+1} Time Series')\n",
    "    plt.ylabel('Signal')\n",
    "    if i == 4 or i == time_series.shape[1]-1:\n",
    "        plt.xlabel('Time Points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save time series data\n",
    "time_series_df = pd.DataFrame(time_series, columns=[f'ROI_{i+1}' for i in range(time_series.shape[1])])\n",
    "time_series_path = os.path.join(output_dir, 'roi_time_series.csv')\n",
    "time_series_df.to_csv(time_series_path, index=False)\n",
    "print(f\"Saved time series data to {time_series_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Connectivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute functional connectivity matrix\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "# Initialize connectivity measure\n",
    "connectivity_measure = ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = connectivity_measure.fit_transform([time_series])[0]\n",
    "\n",
    "print(f\"Correlation matrix shape: {correlation_matrix.shape}\")\n",
    "\n",
    "# Plot connectivity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation Coefficient')\n",
    "plt.title('Functional Connectivity Matrix')\n",
    "plt.xlabel('ROI Index')\n",
    "plt.ylabel('ROI Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save connectivity matrix\n",
    "connectivity_path = os.path.join(output_dir, 'connectivity_matrix.csv')\n",
    "pd.DataFrame(correlation_matrix).to_csv(connectivity_path, index=False)\n",
    "print(f\"Saved connectivity matrix to {connectivity_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload Results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload analysis results to S3\n",
    "def upload_results_to_s3(local_dir, s3_bucket, s3_prefix):\n",
    "    \"\"\"\n",
    "    Upload analysis results to S3\n",
    "    \n",
    "    Parameters:\n",
    "    local_dir (str): Local directory containing results\n",
    "    s3_bucket (str): S3 bucket name\n",
    "    s3_prefix (str): S3 prefix for uploaded files\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, local_dir)\n",
    "            s3_key = f\"{s3_prefix}/{relative_path}\"\n",
    "            \n",
    "            try:\n",
    "                s3_client.upload_file(local_path, s3_bucket, s3_key)\n",
    "                print(f\"Uploaded {local_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {local_path}: {e}\")\n",
    "\n",
    "# Upload results\n",
    "print(\"Uploading results to S3...\")\n",
    "upload_results_to_s3(output_dir, bucket, f\"{prefix}/results\")\n",
    "print(\"Upload completed!\")\n",
    "\n",
    "# Generate summary report\n",
    "summary_report = f\"\"\"\n",
    "# fMRI Analysis Summary Report\n",
    "\n",
    "## Analysis Details\n",
    "- Subject ID: {subject_id}\n",
    "- Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- Region: {region}\n",
    "- S3 Bucket: {bucket}\n",
    "\n",
    "## Data Information\n",
    "- fMRI Image Shape: {fmri_img.shape}\n",
    "- Number of Time Points: {n_scans}\n",
    "- Number of ROIs: {time_series.shape[1]}\n",
    "\n",
    "## Generated Files\n",
    "- Statistical maps: stat_map_*.nii.gz\n",
    "- ROI time series: roi_time_series.csv\n",
    "- Connectivity matrix: connectivity_matrix.csv\n",
    "\n",
    "## S3 Location\n",
    "s3://{bucket}/{prefix}/results/\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "report_path = os.path.join(output_dir, 'analysis_summary.md')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(summary_report)"
   ]
  }
 ]
}