{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkinson's Disease Detection from fMRI Data on AWS SageMaker\n",
    "\n",
    "This notebook implements a machine learning pipeline for detecting Parkinson's disease from functional MRI (fMRI) data.\n",
    "Based on neuroimaging research methodologies for movement disorder classification.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Classify Parkinson's disease patients vs. healthy controls using fMRI features\n",
    "- **Approach**: Extract functional connectivity and regional activity features\n",
    "- **Methods**: Support Vector Machine, Random Forest, and Deep Learning classifiers\n",
    "- **Validation**: Cross-validation with performance metrics\n",
    "\n",
    "## Prerequisites\n",
    "- AWS SageMaker notebook instance in us-east-2 region\n",
    "- fMRI data stored in S3 (preprocessed with fMRIPrep)\n",
    "- Required libraries: nilearn, scikit-learn, tensorflow, nibabel\n",
    "\n",
    "## Dataset Structure Expected\n",
    "```\n",
    "s3://bucket/datasets/\n",
    "├── controls/\n",
    "│   ├── sub-001_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "│   └── sub-002_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "└── patients/\n",
    "    ├── sub-101_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "    └── sub-102_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nilearn\n",
      "  Downloading nilearn-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.16.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nilearn) (3.1.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nilearn) (1.5.2)\n",
      "Collecting nibabel>=5.2.0 (from nilearn)\n",
      "  Downloading nibabel-5.3.3-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nilearn) (24.2)\n",
      "Requirement already satisfied: requests>=2.30.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nilearn) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nilearn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (4.25.8)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.30.0->nilearn) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.30.0->nilearn) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.30.0->nilearn) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.30.0->nilearn) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jinja2>=3.1.2->nilearn) (3.0.3)\n",
      "Requirement already satisfied: rich in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nibabel>=5.2.0->nilearn) (6.5.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Downloading nilearn-0.13.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nibabel-5.3.3-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m162.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nibabel, nilearn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [nilearn]m1/2\u001b[0m [nilearn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nibabel-5.3.3 nilearn-0.13.0\n",
      "Requirement already satisfied: nibabel in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (5.3.3)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.42.16)\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.255.0)\n",
      "Requirement already satisfied: plotly in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (6.4.0)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.14.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nibabel) (6.5.2)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nibabel) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nibabel) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nibabel) (4.15.0)\n",
      "Requirement already satisfied: botocore<1.43.0,>=1.42.16 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (1.42.16)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.43.0,>=1.42.16->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.43.0,>=1.42.16->boto3) (1.26.20)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.43.0,>=1.42.16->boto3) (1.17.0)\n",
      "Requirement already satisfied: attrs<26,>=24 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (25.4.0)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (3.1.2)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.121.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: graphene<4,>=3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (3.4.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.25.1)\n",
      "Requirement already satisfied: omegaconf<3,>=2.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (2.3.0)\n",
      "Requirement already satisfied: pandas>=2.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (2.3.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.3.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.5.0)\n",
      "Requirement already satisfied: protobuf<6.32,>=3.12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.25.8)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (7.1.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (6.0.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (2.32.5)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.71 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.0.72)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.7.8)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.67.1)\n",
      "Requirement already satisfied: uvicorn in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.38.0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker) (3.2.7)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.23.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from omegaconf<3,>=2.2->sagemaker) (4.9.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.71->sagemaker) (2.12.3)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.71->sagemaker) (14.2.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.71->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.28.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.71->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.71->sagemaker) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.71->sagemaker) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.71->sagemaker) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.71->sagemaker) (2.19.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from plotly) (2.10.2)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from imbalanced-learn) (1.7.2)\n",
      "Collecting sklearn-compat<0.2,>=0.1.5 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from imbalanced-learn) (3.6.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.71->sagemaker) (0.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas>=2.3.0->sagemaker) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas>=2.3.0->sagemaker) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->sagemaker) (2025.10.5)\n",
      "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fastapi->sagemaker) (0.49.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fastapi->sagemaker) (0.0.3)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from starlette<0.50.0,>=0.40.0->fastapi->sagemaker) (4.11.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi->sagemaker) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi->sagemaker) (1.3.1)\n",
      "Requirement already satisfied: ppft>=1.7.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.7)\n",
      "Requirement already satisfied: dill>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.4.0)\n",
      "Requirement already satisfied: pox>=0.3.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.18 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.18)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from uvicorn->sagemaker) (8.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from uvicorn->sagemaker) (0.16.0)\n",
      "Downloading imbalanced_learn-0.14.1-py3-none-any.whl (235 kB)\n",
      "Downloading sklearn_compat-0.1.5-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [imbalanced-learn][imbalanced-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed imbalanced-learn-0.14.1 sklearn-compat-0.1.5\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.3.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[49 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_wheel:Parsed build configuration: {'hide_cxx_symbols': True, 'use_openmp': True, 'use_cuda': False, 'use_nccl': False, 'use_dlopen_nccl': False, 'plugin_federated': False, 'plugin_rmm': False, 'use_system_libxgboost': False}\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_wheel:Copying project files to temporary directory /tmp/tmpvp8tcz44/whl_workspace\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_wheel:Copying /tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/pyproject.toml -> /tmp/tmpvp8tcz44/whl_workspace/pyproject.toml\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_wheel:Copying /tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/hatch_build.py -> /tmp/tmpvp8tcz44/whl_workspace/hatch_build.py\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_wheel:Copying /tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/README.rst -> /tmp/tmpvp8tcz44/whl_workspace/README.rst\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_wheel:Copying /tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/xgboost -> /tmp/tmpvp8tcz44/whl_workspace/xgboost\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_libxgboost:Building libxgboost.so from the C++ source files in /tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/cpp_src...\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_libxgboost:CMake args: ['cmake', '/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/cpp_src', '-GUnix Makefiles', '-DKEEP_BUILD_ARTIFACTS_IN_BINARY_DIR=ON', '-DHIDE_CXX_SYMBOLS=ON', '-DUSE_OPENMP=ON', '-DUSE_CUDA=OFF', '-DUSE_NCCL=OFF', '-DUSE_DLOPEN_NCCL=OFF', '-DPLUGIN_FEDERATED=OFF', '-DPLUGIN_RMM=OFF']\n",
      "  \u001b[31m   \u001b[0m CMake Error at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   CMake 3.18 or higher is required.  You are running version 2.8.12.2\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m -- Configuring incomplete, errors occurred!\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_libxgboost:Failed to build with OpenMP. Exception: Command '['cmake', '/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/cpp_src', '-GUnix Makefiles', '-DKEEP_BUILD_ARTIFACTS_IN_BINARY_DIR=ON', '-DHIDE_CXX_SYMBOLS=ON', '-DUSE_OPENMP=ON', '-DUSE_CUDA=OFF', '-DUSE_NCCL=OFF', '-DUSE_DLOPEN_NCCL=OFF', '-DPLUGIN_FEDERATED=OFF', '-DPLUGIN_RMM=OFF']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m INFO:xgboost.packager.build_libxgboost:CMake args: ['cmake', '/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/cpp_src', '-GUnix Makefiles', '-DKEEP_BUILD_ARTIFACTS_IN_BINARY_DIR=ON', '-DHIDE_CXX_SYMBOLS=ON', '-DUSE_OPENMP=OFF', '-DUSE_CUDA=OFF', '-DUSE_NCCL=OFF', '-DUSE_DLOPEN_NCCL=OFF', '-DPLUGIN_FEDERATED=OFF', '-DPLUGIN_RMM=OFF']\n",
      "  \u001b[31m   \u001b[0m CMake Error at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   CMake 3.18 or higher is required.  You are running version 2.8.12.2\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m -- Configuring incomplete, errors occurred!\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/packager/nativelib.py\", line 100, in build_libxgboost\n",
      "  \u001b[31m   \u001b[0m     _build(generator=generator)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/packager/nativelib.py\", line 57, in _build\n",
      "  \u001b[31m   \u001b[0m     subprocess.check_call(cmake_cmd, cwd=build_dir)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/subprocess.py\", line 369, in check_call\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, cmd)\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['cmake', '/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/cpp_src', '-GUnix Makefiles', '-DKEEP_BUILD_ARTIFACTS_IN_BINARY_DIR=ON', '-DHIDE_CXX_SYMBOLS=ON', '-DUSE_OPENMP=ON', '-DUSE_CUDA=OFF', '-DUSE_NCCL=OFF', '-DUSE_DLOPEN_NCCL=OFF', '-DPLUGIN_FEDERATED=OFF', '-DPLUGIN_RMM=OFF']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m During handling of the above exception, another exception occurred:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 178, in prepare_metadata_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     whl_basename = backend.build_wheel(metadata_directory, config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/packager/pep517.py\", line 80, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     libxgboost = locate_or_build_libxgboost(\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/packager/nativelib.py\", line 171, in locate_or_build_libxgboost\n",
      "  \u001b[31m   \u001b[0m     return build_libxgboost(cpp_src_dir, build_dir=build_dir, build_config=build_config)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/packager/nativelib.py\", line 104, in build_libxgboost\n",
      "  \u001b[31m   \u001b[0m     _build(generator=generator)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/packager/nativelib.py\", line 57, in _build\n",
      "  \u001b[31m   \u001b[0m     subprocess.check_call(cmake_cmd, cwd=build_dir)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/subprocess.py\", line 369, in check_call\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, cmd)\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['cmake', '/tmp/pip-install-sna5zfkd/xgboost_d9734a9f6ed84792ae932f65c387c868/cpp_src', '-GUnix Makefiles', '-DKEEP_BUILD_ARTIFACTS_IN_BINARY_DIR=ON', '-DHIDE_CXX_SYMBOLS=ON', '-DUSE_OPENMP=OFF', '-DUSE_CUDA=OFF', '-DUSE_NCCL=OFF', '-DUSE_DLOPEN_NCCL=OFF', '-DPLUGIN_FEDERATED=OFF', '-DPLUGIN_RMM=OFF']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hsagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 13:40:52.961915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-17 13:40:56.871498: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-17 13:40:56.902932: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-17 13:41:02.424338: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-17 13:41:09.658806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "TensorFlow version: 2.16.2\n",
      "Scikit-learn version: 1.7.2\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for Parkinson's detection\n",
    "!pip install nilearn scikit-learn tensorflow pandas numpy matplotlib seaborn\n",
    "!pip install nibabel boto3 sagemaker plotly imbalanced-learn\n",
    "!pip install xgboost lightgbm optuna scipy statsmodels\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sklearn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fMRI and neuroimaging imports\n",
    "import nibabel as nib\n",
    "from nilearn import datasets, plotting, image\n",
    "from nilearn.input_data import NiftiLabelsMasker, NiftiMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.plotting import plot_connectome, plot_matrix\n",
    "from nilearn.glm.first_level import FirstLevelModel, make_first_level_design_matrix\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AWS SageMaker Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker execution role: arn:aws:iam::257967673968:role/parkinson-fmri-notebook-stack-SageMakerNotebookRole-6Dt9LfTmian7\n",
      "Using existing fMRI bucket: fmri-dataset-bucket-257967673968-us-east-2\n",
      "Using S3 bucket: fmri-dataset-bucket-257967673968-us-east-2\n",
      "Data prefix: parkinson-fmri-detection\n"
     ]
    }
   ],
   "source": [
    "# Configure AWS region and SageMaker session\n",
    "region = 'us-east-2'\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "# Get SageMaker execution role\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(f\"SageMaker execution role: {role}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get execution role: {e}\")\n",
    "    # For notebook instances, we can get the role from instance metadata\n",
    "    import urllib.request\n",
    "    import json\n",
    "    try:\n",
    "        # Get instance profile\n",
    "        response = urllib.request.urlopen('http://169.254.169.254/latest/meta-data/iam/security-credentials/')\n",
    "        role_name = response.read().decode('utf-8')\n",
    "        role = f\"arn:aws:iam::{boto3.client('sts').get_caller_identity()['Account']}:role/{role_name}\"\n",
    "        print(f\"Using role from instance metadata: {role}\")\n",
    "    except:\n",
    "        role = None\n",
    "        print(\"Could not determine execution role\")\n",
    "\n",
    "# Set up S3 bucket for data storage\n",
    "#try:\n",
    "#    bucket = sagemaker_session.default_bucket()\n",
    "#except:\n",
    "# If default bucket fails, try to find existing buckets\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "buckets = s3_client.list_buckets()['Buckets']\n",
    "fmri_buckets = [b['Name'] for b in buckets if 'fmri' in b['Name'].lower()]\n",
    "if fmri_buckets:\n",
    "    bucket = fmri_buckets[0]\n",
    "    print(f\"Using existing fMRI bucket: {bucket}\")\n",
    "else:\n",
    "    bucket = f\"sagemaker-{region}-{boto3.client('sts').get_caller_identity()['Account']}\"\n",
    "    print(f\"Using default bucket pattern: {bucket}\")\n",
    "\n",
    "prefix = 'parkinson-fmri-detection'\n",
    "\n",
    "print(f\"Using S3 bucket: {bucket}\")\n",
    "print(f\"Data prefix: {prefix}\")\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/ec2-user/SageMaker/data\n",
      "Output directory: /home/ec2-user/SageMaker/output\n",
      "Models directory: /home/ec2-user/SageMaker/models\n"
     ]
    }
   ],
   "source": [
    "# Define data paths\n",
    "data_dir = '/home/ec2-user/SageMaker/data'  # SageMaker notebook instance path\n",
    "output_dir = '/home/ec2-user/SageMaker/output'\n",
    "models_dir = '/home/ec2-user/SageMaker/models'\n",
    "\n",
    "# Create directories\n",
    "for directory in [data_dir, output_dir, models_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in S3. Creating sample dataset for demonstration...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_neurovault_motor_task</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Added README.md to <span style=\"color: #e100e1; text-decoration-color: #e100e1\">/home/ec2-user/nilearn_data</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mfetch_neurovault_motor_task\u001b[0m\u001b[1;34m]\u001b[0m Added README.md to \u001b[38;2;225;0;225m/home/ec2-user/\u001b[0m\u001b[38;2;225;0;225mnilearn_data\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_neurovault_motor_task</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Dataset created in <span style=\"color: #e100e1; text-decoration-color: #e100e1\">/home/ec2-user/nilearn_data/neurovault</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mfetch_neurovault_motor_task\u001b[0m\u001b[1;34m]\u001b[0m Dataset created in \u001b[38;2;225;0;225m/home/ec2-user/nilearn_data/\u001b[0m\u001b[38;2;225;0;225mneurovault\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample dataset with 1 subjects\n",
      "\n",
      "Dataset Summary:\n",
      "Total subjects: 1\n",
      "Controls: 1\n",
      "Patients: 0\n",
      "\n",
      "Dataset DataFrame:\n",
      "   subject_id                                          file_path  label  \\\n",
      "0  sample_000  /home/ec2-user/SageMaker/data/sample_sub-000.n...      0   \n",
      "\n",
      "     group  \n",
      "0  Control  \n",
      "\n",
      "Class distribution:\n",
      "group\n",
      "Control    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_fmri_dataset(s3_bucket, s3_prefix='datasets'):\n",
    "    \"\"\"\n",
    "    Load fMRI dataset from S3 with labels (controls vs patients)\n",
    "    \n",
    "    Expected S3 structure:\n",
    "    s3://bucket/datasets/controls/sub-*.nii.gz\n",
    "    s3://bucket/datasets/patients/sub-*.nii.gz\n",
    "    \n",
    "    Returns:\n",
    "    file_paths: List of local file paths\n",
    "    labels: List of labels (0=control, 1=patient)\n",
    "    subject_ids: List of subject identifiers\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    subject_ids = []\n",
    "    \n",
    "    # Download controls data\n",
    "    try:\n",
    "        controls_objects = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket, \n",
    "            Prefix=f'{s3_prefix}/controls/'\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in controls_objects:\n",
    "            for obj in controls_objects['Contents']:\n",
    "                if obj['Key'].endswith('.nii.gz'):\n",
    "                    local_path = os.path.join(data_dir, 'controls', os.path.basename(obj['Key']))\n",
    "                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "                    \n",
    "                    s3_client.download_file(s3_bucket, obj['Key'], local_path)\n",
    "                    file_paths.append(local_path)\n",
    "                    labels.append(0)  # Control\n",
    "                    \n",
    "                    # Extract subject ID\n",
    "                    filename = os.path.basename(obj['Key'])\n",
    "                    subject_id = filename.split('_')[0].replace('sub-', '')\n",
    "                    subject_ids.append(f'control_{subject_id}')\n",
    "                    \n",
    "            print(f\"Downloaded {len([l for l in labels if l == 0])} control subjects\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading controls: {e}\")\n",
    "    \n",
    "    # Download patients data\n",
    "    try:\n",
    "        patients_objects = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket, \n",
    "            Prefix=f'{s3_prefix}/patients/'\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in patients_objects:\n",
    "            for obj in patients_objects['Contents']:\n",
    "                if obj['Key'].endswith('.nii.gz'):\n",
    "                    local_path = os.path.join(data_dir, 'patients', os.path.basename(obj['Key']))\n",
    "                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "                    \n",
    "                    s3_client.download_file(s3_bucket, obj['Key'], local_path)\n",
    "                    file_paths.append(local_path)\n",
    "                    labels.append(1)  # Patient\n",
    "                    \n",
    "                    # Extract subject ID\n",
    "                    filename = os.path.basename(obj['Key'])\n",
    "                    subject_id = filename.split('_')[0].replace('sub-', '')\n",
    "                    subject_ids.append(f'patient_{subject_id}')\n",
    "                    \n",
    "            print(f\"Downloaded {len([l for l in labels if l == 1])} patient subjects\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading patients: {e}\")\n",
    "    \n",
    "    # If no real data available, create sample data for demonstration\n",
    "    if len(file_paths) == 0:\n",
    "        print(\"No data found in S3. Creating sample dataset for demonstration...\")\n",
    "        \n",
    "        try:\n",
    "            # Load sample motor task data from nilearn\n",
    "            motor_images = datasets.fetch_neurovault_motor_task()\n",
    "            \n",
    "            # Use first few images as controls and patients\n",
    "            for i, img_path in enumerate(motor_images.images[:10]):\n",
    "                local_path = os.path.join(data_dir, f'sample_sub-{i:03d}.nii.gz')\n",
    "                # Copy the file\n",
    "                import shutil\n",
    "                shutil.copy(img_path, local_path)\n",
    "                \n",
    "                file_paths.append(local_path)\n",
    "                labels.append(i % 2)  # Alternate between control (0) and patient (1)\n",
    "                subject_ids.append(f'sample_{i:03d}')\n",
    "            \n",
    "            print(f\"Created sample dataset with {len(file_paths)} subjects\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating sample dataset: {e}\")\n",
    "            # Create minimal dummy data\n",
    "            print(\"Creating minimal dummy dataset...\")\n",
    "            for i in range(4):\n",
    "                # Create dummy NIfTI files\n",
    "                dummy_data = np.random.randn(64, 64, 30, 100)  # Small 4D image\n",
    "                dummy_img = nib.Nifti1Image(dummy_data, affine=np.eye(4))\n",
    "                local_path = os.path.join(data_dir, f'dummy_sub-{i:03d}.nii.gz')\n",
    "                dummy_img.to_filename(local_path)\n",
    "                \n",
    "                file_paths.append(local_path)\n",
    "                labels.append(i % 2)\n",
    "                subject_ids.append(f'dummy_{i:03d}')\n",
    "            \n",
    "            print(f\"Created dummy dataset with {len(file_paths)} subjects\")\n",
    "    \n",
    "    return file_paths, labels, subject_ids\n",
    "\n",
    "# Load the dataset\n",
    "file_paths, labels, subject_ids = load_fmri_dataset(bucket, prefix)\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Total subjects: {len(file_paths)}\")\n",
    "print(f\"Controls: {labels.count(0)}\")\n",
    "print(f\"Patients: {labels.count(1)}\")\n",
    "\n",
    "# Create DataFrame for easier handling\n",
    "dataset_df = pd.DataFrame({\n",
    "    'subject_id': subject_ids,\n",
    "    'file_path': file_paths,\n",
    "    'label': labels,\n",
    "    'group': ['Control' if l == 0 else 'Patient' for l in labels]\n",
    "})\n",
    "\n",
    "print(\"\\nDataset DataFrame:\")\n",
    "print(dataset_df.head())\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(dataset_df['group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction from fMRI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading brain atlas...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_atlas_harvard_oxford</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Dataset created in <span style=\"color: #e100e1; text-decoration-color: #e100e1\">/home/ec2-user/nilearn_data/fsl</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mfetch_atlas_harvard_oxford\u001b[0m\u001b[1;34m]\u001b[0m Dataset created in \u001b[38;2;225;0;225m/home/ec2-user/nilearn_data/\u001b[0m\u001b[38;2;225;0;225mfsl\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_atlas_harvard_oxford</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloading data from <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://www.nitrc.org/frs/download.php/9902/HarvardOxford.tgz</span> \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mfetch_atlas_harvard_oxford\u001b[0m\u001b[1;34m]\u001b[0m Downloading data from \u001b[4;38;2;0;105;255mhttps://www.nitrc.org/frs/download.php/9902/HarvardOxford.tgz\u001b[0m \n",
       "\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_atlas_harvard_oxford</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span>  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>done. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> seconds, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> min<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mfetch_atlas_harvard_oxford\u001b[0m\u001b[1;34m]\u001b[0m  \u001b[33m...\u001b[0mdone. \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m seconds, \u001b[1;36m0\u001b[0m min\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_atlas_harvard_oxford</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Extracting data from \n",
       "<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/home/ec2-user/nilearn_data/fsl/5c734f16e50cc772ef593cab9bb3137b/HarvardOxford.tgz...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mfetch_atlas_harvard_oxford\u001b[0m\u001b[1;34m]\u001b[0m Extracting data from \n",
       "\u001b[38;2;225;0;225m/home/ec2-user/nilearn_data/fsl/5c734f16e50cc772ef593cab9bb3137b/\u001b[0m\u001b[38;2;225;0;225mHarvardOxford.tgz...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_atlas_harvard_oxford</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> .. done.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mfetch_atlas_harvard_oxford\u001b[0m\u001b[1;34m]\u001b[0m .. done.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas loaded with 48 regions\n",
      "First 10 regions: ['Frontal Pole', 'Insular Cortex', 'Superior Frontal Gyrus', 'Middle Frontal Gyrus', 'Inferior Frontal Gyrus, pars triangularis', 'Inferior Frontal Gyrus, pars opercularis', 'Precentral Gyrus', 'Temporal Pole', 'Superior Temporal Gyrus, anterior division', 'Superior Temporal Gyrus, posterior division']\n",
      "ROI masker initialized\n"
     ]
    }
   ],
   "source": [
    "# Load brain atlas for ROI extraction\n",
    "print(\"Loading brain atlas...\")\n",
    "\n",
    "try:\n",
    "    # Use Harvard-Oxford atlas for ROI definition\n",
    "    atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "    atlas_labels = atlas.labels[1:]  # Remove background label\n",
    "    \n",
    "    print(f\"Atlas loaded with {len(atlas_labels)} regions\")\n",
    "    print(f\"First 10 regions: {atlas_labels[:10]}\")\n",
    "    \n",
    "    # Initialize masker for ROI extraction\n",
    "    masker = NiftiLabelsMasker(\n",
    "        labels_img=atlas.maps,\n",
    "        standardize=True,\n",
    "        memory='nilearn_cache',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"ROI masker initialized\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading atlas: {e}\")\n",
    "    print(\"Using simplified approach with whole-brain masker...\")\n",
    "    \n",
    "    # Fallback to whole-brain masker\n",
    "    masker = NiftiMasker(\n",
    "        standardize=True,\n",
    "        memory='nilearn_cache',\n",
    "        verbose=0\n",
    "    )\n",
    "    atlas_labels = [f'Region_{i}' for i in range(100)]  # Dummy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from all subjects...\n",
      "Processing 1/1: sample_000\n",
      "Error processing /home/ec2-user/SageMaker/data/sample_sub-000.nii.gz: tuple index out of range\n",
      "Skipping sample_000 due to processing error\n",
      "No features extracted. Please check your data.\n"
     ]
    }
   ],
   "source": [
    "def extract_features_from_fmri(file_path, masker):\n",
    "    \"\"\"\n",
    "    Extract features from a single fMRI file\n",
    "    \n",
    "    Features extracted:\n",
    "    1. ROI time series mean and std\n",
    "    2. Functional connectivity matrix (correlation)\n",
    "    3. Regional homogeneity measures\n",
    "    \n",
    "    Returns:\n",
    "    features: 1D numpy array of features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load fMRI image\n",
    "        img = nib.load(file_path)\n",
    "        \n",
    "        # Extract time series from ROIs\n",
    "        time_series = masker.fit_transform(img)\n",
    "        \n",
    "        # Ensure we have a reasonable number of features\n",
    "        if time_series.shape[1] > 1000:\n",
    "            # Reduce dimensionality if too many voxels\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA(n_components=100)\n",
    "            time_series = pca.fit_transform(time_series.T).T\n",
    "        \n",
    "        # Feature 1: Statistical measures of time series\n",
    "        roi_means = np.mean(time_series, axis=0)\n",
    "        roi_stds = np.std(time_series, axis=0)\n",
    "        roi_vars = np.var(time_series, axis=0)\n",
    "        \n",
    "        # Feature 2: Functional connectivity (correlation matrix)\n",
    "        connectivity_measure = ConnectivityMeasure(kind='correlation')\n",
    "        correlation_matrix = connectivity_measure.fit_transform([time_series])[0]\n",
    "        \n",
    "        # Extract upper triangle of correlation matrix (avoid redundancy)\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "        connectivity_features = correlation_matrix[mask]\n",
    "        \n",
    "        # Feature 3: Network measures\n",
    "        # Mean connectivity strength per ROI\n",
    "        connectivity_strength = np.mean(np.abs(correlation_matrix), axis=1)\n",
    "        \n",
    "        # Feature 4: Simple frequency domain features\n",
    "        freq_features = []\n",
    "        for roi_ts in time_series.T[:min(10, time_series.shape[1])]:\n",
    "            # Simple power measures\n",
    "            fft_vals = np.fft.fft(roi_ts)\n",
    "            power_spectrum = np.abs(fft_vals)**2\n",
    "            \n",
    "            # Extract power in different frequency bands\n",
    "            low_freq_power = np.mean(power_spectrum[:len(power_spectrum)//4])\n",
    "            high_freq_power = np.mean(power_spectrum[len(power_spectrum)//4:])\n",
    "            \n",
    "            freq_features.extend([low_freq_power, high_freq_power])\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = np.concatenate([\n",
    "            roi_means,\n",
    "            roi_stds,\n",
    "            roi_vars,\n",
    "            connectivity_features,\n",
    "            connectivity_strength,\n",
    "            freq_features\n",
    "        ])\n",
    "        \n",
    "        # Handle any NaN or infinite values\n",
    "        all_features = np.nan_to_num(all_features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        return all_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract features from all subjects\n",
    "print(\"Extracting features from all subjects...\")\n",
    "features_list = []\n",
    "valid_labels = []\n",
    "valid_subjects = []\n",
    "\n",
    "for i, (file_path, label, subject_id) in enumerate(zip(file_paths, labels, subject_ids)):\n",
    "    print(f\"Processing {i+1}/{len(file_paths)}: {subject_id}\")\n",
    "    \n",
    "    features = extract_features_from_fmri(file_path, masker)\n",
    "    \n",
    "    if features is not None:\n",
    "        features_list.append(features)\n",
    "        valid_labels.append(label)\n",
    "        valid_subjects.append(subject_id)\n",
    "    else:\n",
    "        print(f\"Skipping {subject_id} due to processing error\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "if len(features_list) > 0:\n",
    "    X = np.array(features_list)\n",
    "    y = np.array(valid_labels)\n",
    "    \n",
    "    print(f\"\\nFeature extraction completed!\")\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Number of features per subject: {X.shape[1]}\")\n",
    "    print(f\"Valid subjects: {len(valid_subjects)}\")\n",
    "    print(f\"Controls: {np.sum(y == 0)}, Patients: {np.sum(y == 1)}\")\n",
    "else:\n",
    "    print(\"No features extracted. Please check your data.\")\n",
    "    X, y = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None and len(X) > 1:\n",
    "    # Prepare data for machine learning\n",
    "    print(\"Preparing data for machine learning...\")\n",
    "    \n",
    "    # Handle missing values (if any)\n",
    "    X_clean = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Feature selection - keep top features by variance\n",
    "    feature_vars = np.var(X_clean, axis=0)\n",
    "    top_features_idx = np.argsort(feature_vars)[-min(100, X_clean.shape[1]):]\n",
    "    X_selected = X_clean[:, top_features_idx]\n",
    "    \n",
    "    print(f\"Selected feature matrix shape: {X_selected.shape}\")\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    if len(np.unique(y)) > 1 and len(X_selected) > 2:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_selected, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "        print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "        print(f\"Training set distribution: {np.bincount(y_train)}\")\n",
    "        print(f\"Test set distribution: {np.bincount(y_test)}\")\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        print(\"Data preprocessing completed\")\n",
    "        \n",
    "        # Define and train classifiers\n",
    "        classifiers = {\n",
    "            'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        # Train and evaluate classifiers\n",
    "        results = {}\n",
    "        trained_models = {}\n",
    "        \n",
    "        print(\"Training and evaluating classifiers...\")\n",
    "        \n",
    "        for name, clf in classifiers.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Train the classifier\n",
    "                clf.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = clf.predict(X_test_scaled)\n",
    "                y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = clf.score(X_test_scaled, y_test)\n",
    "                auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "                \n",
    "                # Cross-validation (if enough samples)\n",
    "                if len(X_train_scaled) >= 5:\n",
    "                    cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=min(3, len(X_train_scaled)), scoring='accuracy')\n",
    "                    cv_mean = cv_scores.mean()\n",
    "                    cv_std = cv_scores.std()\n",
    "                else:\n",
    "                    cv_mean = accuracy\n",
    "                    cv_std = 0.0\n",
    "                \n",
    "                results[name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'auc': auc_score,\n",
    "                    'cv_mean': cv_mean,\n",
    "                    'cv_std': cv_std,\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_pred_proba\n",
    "                }\n",
    "                \n",
    "                trained_models[name] = clf\n",
    "                \n",
    "                print(f\"{name} Results:\")\n",
    "                print(f\"  Test Accuracy: {accuracy:.3f}\")\n",
    "                print(f\"  AUC Score: {auc_score:.3f}\")\n",
    "                print(f\"  CV Accuracy: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
    "                \n",
    "                # Classification report\n",
    "                print(f\"\\nClassification Report for {name}:\")\n",
    "                print(classification_report(y_test, y_pred, target_names=['Control', 'Patient']))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name}: {e}\")\n",
    "        \n",
    "        print(\"\\nClassifier training completed!\")\n",
    "        \n",
    "        # Plot ROC curves if we have results\n",
    "        if results:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            colors = ['blue', 'red', 'green', 'orange']\n",
    "            \n",
    "            for i, (model_name, result) in enumerate(results.items()):\n",
    "                fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "                auc_score = result['auc']\n",
    "                \n",
    "                plt.plot(fpr, tpr, color=colors[i % len(colors)], lw=2,\n",
    "                         label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "            \n",
    "            # Plot diagonal line\n",
    "            plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.8)\n",
    "            \n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC Curves - Parkinson\\'s Disease Detection')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Save ROC plot\n",
    "            plt.savefig(os.path.join(output_dir, 'roc_curves.png'), dpi=300, bbox_inches='tight')\n",
    "            print(\"ROC curves plot saved\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Not enough samples or classes for train/test split\")\n",
    "else:\n",
    "    print(\"No valid features extracted for machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "from datetime import datetime\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "# Parkinson's Disease fMRI Detection - Analysis Summary Report\n",
    "\n",
    "## Analysis Details\n",
    "- **Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **AWS Region**: {region}\n",
    "- **S3 Bucket**: {bucket}\n",
    "- **Data Prefix**: {prefix}\n",
    "\n",
    "## Dataset Information\n",
    "- **Total Subjects**: {len(valid_subjects) if 'valid_subjects' in locals() else 'N/A'}\n",
    "- **Controls**: {np.sum(y == 0) if 'y' in locals() and y is not None else 'N/A'}\n",
    "- **Patients**: {np.sum(y == 1) if 'y' in locals() and y is not None else 'N/A'}\n",
    "- **Features Extracted**: {X.shape[1] if 'X' in locals() and X is not None else 'N/A'}\n",
    "\n",
    "## Model Performance Summary\n",
    "\"\"\"\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    for model_name, result in results.items():\n",
    "        summary_report += f\"\"\"\n",
    "### {model_name}\n",
    "- **Test Accuracy**: {result['accuracy']:.3f}\n",
    "- **AUC Score**: {result['auc']:.3f}\n",
    "- **Cross-Validation**: {result['cv_mean']:.3f} ± {result['cv_std']:.3f}\n",
    "\"\"\"\n",
    "else:\n",
    "    summary_report += \"\\nNo model results available.\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "## Generated Files\n",
    "- ROC curves: roc_curves.png\n",
    "- Analysis summary: analysis_summary.md\n",
    "\n",
    "## S3 Locations\n",
    "- **Results**: s3://{bucket}/{prefix}/results/\n",
    "\n",
    "## Notes\n",
    "- This analysis demonstrates the fMRI-based Parkinson's detection pipeline\n",
    "- Results may vary based on dataset quality and size\n",
    "- For production use, ensure adequate sample sizes and proper validation\n",
    "\n",
    "---\n",
    "*Report generated automatically by Parkinson's fMRI Detection Pipeline*\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "report_path = os.path.join(output_dir, 'analysis_summary.md')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARKINSON'S DISEASE fMRI DETECTION ANALYSIS COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(summary_report)\n",
    "\n",
    "# Upload results to S3\n",
    "def upload_results_to_s3(local_dir, s3_bucket, s3_prefix):\n",
    "    \"\"\"\n",
    "    Upload analysis results to S3\n",
    "    \"\"\"\n",
    "    uploaded_files = []\n",
    "    \n",
    "    try:\n",
    "        for root, dirs, files in os.walk(local_dir):\n",
    "            for file in files:\n",
    "                local_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(local_path, local_dir)\n",
    "                s3_key = f\"{s3_prefix}/results/{relative_path}\"\n",
    "                \n",
    "                try:\n",
    "                    s3_client.upload_file(local_path, s3_bucket, s3_key)\n",
    "                    uploaded_files.append(f\"s3://{s3_bucket}/{s3_key}\")\n",
    "                    print(f\"Uploaded {relative_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading {local_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing directory {local_dir}: {e}\")\n",
    "    \n",
    "    return uploaded_files\n",
    "\n",
    "# Upload all results\n",
    "print(\"\\nUploading results to S3...\")\n",
    "uploaded_files = upload_results_to_s3(output_dir, bucket, prefix)\n",
    "\n",
    "print(f\"\\n🎯 Analysis completed successfully!\")\n",
    "print(f\"📊 Results uploaded to: s3://{bucket}/{prefix}/results/\")\n",
    "print(f\"🧠 Ready for further analysis and validation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
