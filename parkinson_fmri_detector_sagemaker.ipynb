{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkinson's Disease Detection from fMRI Data on AWS SageMaker\n",
    "\n",
    "This notebook implements a machine learning pipeline for detecting Parkinson's disease from functional MRI (fMRI) data.\n",
    "Based on neuroimaging research methodologies for movement disorder classification.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Classify Parkinson's disease patients vs. healthy controls using fMRI features\n",
    "- **Approach**: Extract functional connectivity and regional activity features\n",
    "- **Methods**: Support Vector Machine, Random Forest, and Deep Learning classifiers\n",
    "- **Validation**: Cross-validation with performance metrics\n",
    "\n",
    "## Prerequisites\n",
    "- AWS SageMaker domain in us-east-2 region\n",
    "- fMRI data stored in S3 (preprocessed with fMRIPrep)\n",
    "- Required libraries: nilearn, scikit-learn, tensorflow, nibabel\n",
    "\n",
    "## Dataset Structure Expected\n",
    "```\n",
    "s3://bucket/datasets/\n",
    "‚îú‚îÄ‚îÄ controls/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sub-001_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ sub-002_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "‚îî‚îÄ‚îÄ patients/\n",
    "    ‚îú‚îÄ‚îÄ sub-101_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "    ‚îî‚îÄ‚îÄ sub-102_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Parkinson's detection\n",
    "!pip install nilearn scikit-learn tensorflow pandas numpy matplotlib seaborn\n",
    "!pip install nibabel boto3 sagemaker plotly imbalanced-learn\n",
    "!pip install xgboost lightgbm optuna  # For advanced ML models and hyperparameter tuning\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sklearn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fMRI and neuroimaging imports\n",
    "import nibabel as nib\n",
    "from nilearn import datasets, plotting, image, connectome\n",
    "from nilearn.input_data import NiftiLabelsMasker, NiftiMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.plotting import plot_connectome, plot_matrix\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AWS SageMaker Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS region and SageMaker session\n",
    "region = 'us-east-2'\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "# Get SageMaker execution role\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"SageMaker execution role: {role}\")\n",
    "\n",
    "# Set up S3 bucket for data storage\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'parkinson-fmri-detection'\n",
    "\n",
    "print(f\"Using S3 bucket: {bucket}\")\n",
    "print(f\"Data prefix: {prefix}\")\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = '/opt/ml/processing/input'\n",
    "output_dir = '/opt/ml/processing/output'\n",
    "models_dir = '/opt/ml/processing/models'\n",
    "\n",
    "# Create directories\n",
    "for directory in [data_dir, output_dir, models_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fmri_dataset(s3_bucket, s3_prefix='datasets'):\n",
    "    \"\"\"\n",
    "    Load fMRI dataset from S3 with labels (controls vs patients)\n",
    "    \n",
    "    Expected S3 structure:\n",
    "    s3://bucket/datasets/controls/sub-*.nii.gz\n",
    "    s3://bucket/datasets/patients/sub-*.nii.gz\n",
    "    \n",
    "    Returns:\n",
    "    file_paths: List of local file paths\n",
    "    labels: List of labels (0=control, 1=patient)\n",
    "    subject_ids: List of subject identifiers\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    subject_ids = []\n",
    "    \n",
    "    # Download controls data\n",
    "    try:\n",
    "        controls_objects = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket, \n",
    "            Prefix=f'{s3_prefix}/controls/'\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in controls_objects:\n",
    "            for obj in controls_objects['Contents']:\n",
    "                if obj['Key'].endswith('.nii.gz'):\n",
    "                    local_path = os.path.join(data_dir, 'controls', os.path.basename(obj['Key']))\n",
    "                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "                    \n",
    "                    s3_client.download_file(s3_bucket, obj['Key'], local_path)\n",
    "                    file_paths.append(local_path)\n",
    "                    labels.append(0)  # Control\n",
    "                    \n",
    "                    # Extract subject ID\n",
    "                    filename = os.path.basename(obj['Key'])\n",
    "                    subject_id = filename.split('_')[0].replace('sub-', '')\n",
    "                    subject_ids.append(f'control_{subject_id}')\n",
    "                    \n",
    "            print(f\"Downloaded {len([l for l in labels if l == 0])} control subjects\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading controls: {e}\")\n",
    "    \n",
    "    # Download patients data\n",
    "    try:\n",
    "        patients_objects = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket, \n",
    "            Prefix=f'{s3_prefix}/patients/'\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in patients_objects:\n",
    "            for obj in patients_objects['Contents']:\n",
    "                if obj['Key'].endswith('.nii.gz'):\n",
    "                    local_path = os.path.join(data_dir, 'patients', os.path.basename(obj['Key']))\n",
    "                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "                    \n",
    "                    s3_client.download_file(s3_bucket, obj['Key'], local_path)\n",
    "                    file_paths.append(local_path)\n",
    "                    labels.append(1)  # Patient\n",
    "                    \n",
    "                    # Extract subject ID\n",
    "                    filename = os.path.basename(obj['Key'])\n",
    "                    subject_id = filename.split('_')[0].replace('sub-', '')\n",
    "                    subject_ids.append(f'patient_{subject_id}')\n",
    "                    \n",
    "            print(f\"Downloaded {len([l for l in labels if l == 1])} patient subjects\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading patients: {e}\")\n",
    "    \n",
    "    # If no real data available, create sample data for demonstration\n",
    "    if len(file_paths) == 0:\n",
    "        print(\"No data found in S3. Creating sample dataset for demonstration...\")\n",
    "        \n",
    "        # Load sample motor task data from nilearn\n",
    "        motor_images = datasets.fetch_neurovault_motor_task()\n",
    "        \n",
    "        # Use first few images as controls and patients\n",
    "        for i, img_path in enumerate(motor_images.images[:10]):\n",
    "            local_path = os.path.join(data_dir, f'sample_sub-{i:03d}.nii.gz')\n",
    "            # Copy the file\n",
    "            import shutil\n",
    "            shutil.copy(img_path, local_path)\n",
    "            \n",
    "            file_paths.append(local_path)\n",
    "            labels.append(i % 2)  # Alternate between control (0) and patient (1)\n",
    "            subject_ids.append(f'sample_{i:03d}')\n",
    "        \n",
    "        print(f\"Created sample dataset with {len(file_paths)} subjects\")\n",
    "    \n",
    "    return file_paths, labels, subject_ids\n",
    "\n",
    "# Load the dataset\n",
    "file_paths, labels, subject_ids = load_fmri_dataset(bucket, prefix)\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Total subjects: {len(file_paths)}\")\n",
    "print(f\"Controls: {labels.count(0)}\")\n",
    "print(f\"Patients: {labels.count(1)}\")\n",
    "\n",
    "# Create DataFrame for easier handling\n",
    "dataset_df = pd.DataFrame({\n",
    "    'subject_id': subject_ids,\n",
    "    'file_path': file_paths,\n",
    "    'label': labels,\n",
    "    'group': ['Control' if l == 0 else 'Patient' for l in labels]\n",
    "})\n",
    "\n",
    "print(\"\\nDataset DataFrame:\")\n",
    "print(dataset_df.head())\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(dataset_df['group'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction from fMRI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brain atlas for ROI extraction\n",
    "print(\"Loading brain atlas...\")\n",
    "\n",
    "# Use Harvard-Oxford atlas for ROI definition\n",
    "atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "atlas_labels = atlas.labels[1:]  # Remove background label\n",
    "\n",
    "print(f\"Atlas loaded with {len(atlas_labels)} regions\")\n",
    "print(f\"First 10 regions: {atlas_labels[:10]}\")\n",
    "\n",
    "# Initialize masker for ROI extraction\n",
    "masker = NiftiLabelsMasker(\n",
    "    labels_img=atlas.maps,\n",
    "    standardize=True,\n",
    "    memory='nilearn_cache',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"ROI masker initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_fmri(file_path, masker):\n",
    "    \"\"\"\n",
    "    Extract features from a single fMRI file\n",
    "    \n",
    "    Features extracted:\n",
    "    1. ROI time series mean and std\n",
    "    2. Functional connectivity matrix (correlation)\n",
    "    3. Regional homogeneity measures\n",
    "    \n",
    "    Returns:\n",
    "    features: 1D numpy array of features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load fMRI image\n",
    "        img = nib.load(file_path)\n",
    "        \n",
    "        # Extract time series from ROIs\n",
    "        time_series = masker.fit_transform(img)\n",
    "        \n",
    "        # Feature 1: Statistical measures of time series\n",
    "        roi_means = np.mean(time_series, axis=0)\n",
    "        roi_stds = np.std(time_series, axis=0)\n",
    "        roi_vars = np.var(time_series, axis=0)\n",
    "        \n",
    "        # Feature 2: Functional connectivity (correlation matrix)\n",
    "        connectivity_measure = ConnectivityMeasure(kind='correlation')\n",
    "        correlation_matrix = connectivity_measure.fit_transform([time_series])[0]\n",
    "        \n",
    "        # Extract upper triangle of correlation matrix (avoid redundancy)\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "        connectivity_features = correlation_matrix[mask]\n",
    "        \n",
    "        # Feature 3: Network measures\n",
    "        # Mean connectivity strength per ROI\n",
    "        connectivity_strength = np.mean(np.abs(correlation_matrix), axis=1)\n",
    "        \n",
    "        # Feature 4: Frequency domain features (power spectral density)\n",
    "        freq_features = []\n",
    "        for roi_ts in time_series.T:\n",
    "            # Compute power spectral density\n",
    "            freqs, psd = plt.psd(roi_ts, NFFT=64, Fs=1.0, return_line=False)\n",
    "            plt.close()  # Close the plot\n",
    "            \n",
    "            # Extract power in different frequency bands\n",
    "            low_freq_power = np.mean(psd[freqs < 0.1])  # < 0.1 Hz\n",
    "            high_freq_power = np.mean(psd[freqs > 0.1])  # > 0.1 Hz\n",
    "            \n",
    "            freq_features.extend([low_freq_power, high_freq_power])\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = np.concatenate([\n",
    "            roi_means,\n",
    "            roi_stds,\n",
    "            roi_vars,\n",
    "            connectivity_features,\n",
    "            connectivity_strength,\n",
    "            freq_features\n",
    "        ])\n",
    "        \n",
    "        return all_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract features from all subjects\n",
    "print(\"Extracting features from all subjects...\")\n",
    "features_list = []\n",
    "valid_labels = []\n",
    "valid_subjects = []\n",
    "\n",
    "for i, (file_path, label, subject_id) in enumerate(zip(file_paths, labels, subject_ids)):\n",
    "    print(f\"Processing {i+1}/{len(file_paths)}: {subject_id}\")\n",
    "    \n",
    "    features = extract_features_from_fmri(file_path, masker)\n",
    "    \n",
    "    if features is not None:\n",
    "        features_list.append(features)\n",
    "        valid_labels.append(label)\n",
    "        valid_subjects.append(subject_id)\n",
    "    else:\n",
    "        print(f\"Skipping {subject_id} due to processing error\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(features_list)\n",
    "y = np.array(valid_labels)\n",
    "\n",
    "print(f\"\\nFeature extraction completed!\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features per subject: {X.shape[1]}\")\n",
    "print(f\"Valid subjects: {len(valid_subjects)}\")\n",
    "print(f\"Controls: {np.sum(y == 0)}, Patients: {np.sum(y == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature DataFrame for analysis\n",
    "feature_names = (\n",
    "    [f'roi_mean_{i}' for i in range(len(atlas_labels))] +\n",
    "    [f'roi_std_{i}' for i in range(len(atlas_labels))] +\n",
    "    [f'roi_var_{i}' for i in range(len(atlas_labels))] +\n",
    "    [f'connectivity_{i}' for i in range(len(atlas_labels) * (len(atlas_labels) - 1) // 2)] +\n",
    "    [f'conn_strength_{i}' for i in range(len(atlas_labels))] +\n",
    "    [f'freq_feature_{i}' for i in range(len(atlas_labels) * 2)]\n",
    ")\n",
    "\n",
    "# Ensure feature names match the actual number of features\n",
    "if len(feature_names) != X.shape[1]:\n",
    "    feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "features_df = pd.DataFrame(X, columns=feature_names)\n",
    "features_df['label'] = y\n",
    "features_df['group'] = ['Control' if l == 0 else 'Patient' for l in y]\n",
    "features_df['subject_id'] = valid_subjects\n",
    "\n",
    "print(\"Feature DataFrame created\")\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot distribution of first 20 features\n",
    "n_features_to_plot = min(20, X.shape[1])\n",
    "\n",
    "for i in range(n_features_to_plot):\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    \n",
    "    # Separate data by group\n",
    "    control_data = X[y == 0, i]\n",
    "    patient_data = X[y == 1, i]\n",
    "    \n",
    "    plt.hist(control_data, alpha=0.7, label='Control', bins=10, color='blue')\n",
    "    plt.hist(patient_data, alpha=0.7, label='Patient', bins=10, color='red')\n",
    "    \n",
    "    plt.title(f'Feature {i+1}', fontsize=8)\n",
    "    plt.xlabel('Value', fontsize=6)\n",
    "    plt.ylabel('Frequency', fontsize=6)\n",
    "    plt.legend(fontsize=6)\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions: Controls vs Patients', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(output_dir, 'feature_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"Feature distribution plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of features\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Performing statistical analysis of features...\")\n",
    "\n",
    "# Perform t-tests for each feature\n",
    "p_values = []\n",
    "t_statistics = []\n",
    "effect_sizes = []\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    control_data = X[y == 0, i]\n",
    "    patient_data = X[y == 1, i]\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat, p_val = stats.ttest_ind(control_data, patient_data)\n",
    "    \n",
    "    # Calculate Cohen's d (effect size)\n",
    "    pooled_std = np.sqrt(((len(control_data) - 1) * np.var(control_data, ddof=1) + \n",
    "                         (len(patient_data) - 1) * np.var(patient_data, ddof=1)) / \n",
    "                        (len(control_data) + len(patient_data) - 2))\n",
    "    cohens_d = (np.mean(control_data) - np.mean(patient_data)) / pooled_std\n",
    "    \n",
    "    p_values.append(p_val)\n",
    "    t_statistics.append(t_stat)\n",
    "    effect_sizes.append(abs(cohens_d))\n",
    "\n",
    "# Apply multiple comparison correction (Bonferroni)\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "corrected_p_values = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Create results DataFrame\n",
    "stats_results = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    't_statistic': t_statistics,\n",
    "    'p_value': p_values,\n",
    "    'corrected_p_value': corrected_p_values,\n",
    "    'effect_size': effect_sizes,\n",
    "    'significant': corrected_p_values < 0.05\n",
    "})\n",
    "\n",
    "# Sort by effect size\n",
    "stats_results = stats_results.sort_values('effect_size', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 most discriminative features:\")\n",
    "print(stats_results.head(10)[['feature', 'effect_size', 'corrected_p_value', 'significant']])\n",
    "\n",
    "# Save statistical results\n",
    "stats_results.to_csv(os.path.join(output_dir, 'feature_statistics.csv'), index=False)\n",
    "print(\"\\nFeature statistics saved to CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Machine Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "print(\"Preparing data for machine learning...\")\n",
    "\n",
    "# Handle missing values (if any)\n",
    "X_clean = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Feature selection based on statistical significance\n",
    "significant_features = stats_results[stats_results['significant']].index.tolist()\n",
    "\n",
    "if len(significant_features) > 0:\n",
    "    print(f\"Using {len(significant_features)} statistically significant features\")\n",
    "    X_selected = X_clean[:, significant_features]\n",
    "else:\n",
    "    print(\"No statistically significant features found. Using top 100 features by effect size.\")\n",
    "    top_features = stats_results.head(min(100, len(stats_results))).index.tolist()\n",
    "    X_selected = X_clean[:, top_features]\n",
    "\n",
    "print(f\"Selected feature matrix shape: {X_selected.shape}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training set distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test set distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance with SMOTE (if needed)\n",
    "class_counts = np.bincount(y_train)\n",
    "imbalance_ratio = max(class_counts) / min(class_counts)\n",
    "\n",
    "print(f\"Class imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio > 1.5:  # Apply SMOTE if imbalanced\n",
    "    print(\"Applying SMOTE to balance classes...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "    print(f\"Balanced training set: {X_train_balanced.shape[0]} samples\")\n",
    "    print(f\"Balanced distribution: {np.bincount(y_train_balanced)}\")\n",
    "else:\n",
    "    X_train_balanced = X_train_scaled\n",
    "    y_train_balanced = y_train\n",
    "    print(\"Classes are reasonably balanced, no SMOTE applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train multiple classifiers\n",
    "classifiers = {\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"Training and evaluating classifiers...\")\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    clf.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = clf.score(X_test_scaled, y_test)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train_balanced, y_train_balanced, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = clf\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  AUC Score: {auc_score:.3f}\")\n",
    "    print(f\"  CV Accuracy: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Control', 'Patient']))\n",
    "\n",
    "print(\"\\nClassifier training completed!\")"
   ]
  }
 ]
}
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deep Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build deep neural network for classification\n",
    "def create_dnn_model(input_dim, hidden_layers=[128, 64, 32], dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Create a deep neural network for binary classification\n",
    "    \n",
    "    Parameters:\n",
    "    input_dim: Number of input features\n",
    "    hidden_layers: List of hidden layer sizes\n",
    "    dropout_rate: Dropout rate for regularization\n",
    "    \n",
    "    Returns:\n",
    "    model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(hidden_layers[0], input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train deep learning model\n",
    "print(\"Creating deep neural network...\")\n",
    "\n",
    "dnn_model = create_dnn_model(X_train_balanced.shape[1])\n",
    "print(dnn_model.summary())\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training deep neural network...\")\n",
    "\n",
    "history = dnn_model.fit(\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate deep learning model\n",
    "dnn_predictions = (dnn_model.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
    "dnn_probabilities = dnn_model.predict(X_test_scaled).flatten()\n",
    "\n",
    "dnn_accuracy = np.mean(dnn_predictions == y_test)\n",
    "dnn_auc = roc_auc_score(y_test, dnn_probabilities)\n",
    "\n",
    "results['Deep Neural Network'] = {\n",
    "    'accuracy': dnn_accuracy,\n",
    "    'auc': dnn_auc,\n",
    "    'predictions': dnn_predictions,\n",
    "    'probabilities': dnn_probabilities\n",
    "}\n",
    "\n",
    "trained_models['Deep Neural Network'] = dnn_model\n",
    "\n",
    "print(f\"\\nDeep Neural Network Results:\")\n",
    "print(f\"  Test Accuracy: {dnn_accuracy:.3f}\")\n",
    "print(f\"  AUC Score: {dnn_auc:.3f}\")\n",
    "print(f\"\\nClassification Report for Deep Neural Network:\")\n",
    "print(classification_report(y_test, dnn_predictions, target_names=['Control', 'Patient']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"Model Comparison Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[model]['accuracy'] for model in results.keys()],\n",
    "    'AUC Score': [results[model]['auc'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "# Add CV scores for traditional ML models\n",
    "cv_means = []\n",
    "cv_stds = []\n",
    "for model in results.keys():\n",
    "    if 'cv_mean' in results[model]:\n",
    "        cv_means.append(results[model]['cv_mean'])\n",
    "        cv_stds.append(results[model]['cv_std'])\n",
    "    else:\n",
    "        cv_means.append(np.nan)\n",
    "        cv_stds.append(np.nan)\n",
    "\n",
    "comparison_df['CV Mean'] = cv_means\n",
    "comparison_df['CV Std'] = cv_stds\n",
    "\n",
    "# Sort by AUC score\n",
    "comparison_df = comparison_df.sort_values('AUC Score', ascending=False)\n",
    "\n",
    "print(comparison_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)\n",
    "print(\"\\nModel comparison saved to CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "\n",
    "for i, (model_name, result) in enumerate(results.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "    auc_score = result['auc']\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[i % len(colors)], lw=2,\n",
    "             label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.8)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Parkinson\\'s Disease Detection', fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save ROC plot\n",
    "plt.savefig(os.path.join(output_dir, 'roc_curves.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"ROC curves plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (model_name, result) in enumerate(results.items()):\n",
    "    if i >= 4:  # Only plot first 4 models\n",
    "        break\n",
    "        \n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Control', 'Patient'],\n",
    "                yticklabels=['Control', 'Patient'],\n",
    "                ax=axes[i])\n",
    "    \n",
    "    axes[i].set_title(f'{model_name}\\nAccuracy: {result[\"accuracy\"]:.3f}', fontsize=12)\n",
    "    axes[i].set_xlabel('Predicted', fontsize=10)\n",
    "    axes[i].set_ylabel('Actual', fontsize=10)\n",
    "\n",
    "# Hide empty subplot if less than 4 models\n",
    "if len(results) < 4:\n",
    "    for j in range(len(results), 4):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Confusion Matrices - Parkinson\\'s Disease Detection', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Save confusion matrices plot\n",
    "plt.savefig(os.path.join(output_dir, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"Confusion matrices plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Interpretation and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"Analyzing feature importance...\")\n",
    "\n",
    "# Get feature importance from Random Forest\n",
    "rf_model = trained_models['Random Forest']\n",
    "feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "if len(significant_features) > 0:\n",
    "    important_feature_names = [feature_names[i] for i in significant_features]\n",
    "else:\n",
    "    important_feature_names = [feature_names[i] for i in top_features]\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': important_feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features (Random Forest):\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20_features = importance_df.head(20)\n",
    "\n",
    "plt.barh(range(len(top_20_features)), top_20_features['importance'])\n",
    "plt.yticks(range(len(top_20_features)), top_20_features['feature'], fontsize=8)\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.title('Top 20 Feature Importance (Random Forest)', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance plot\n",
    "plt.savefig(os.path.join(output_dir, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"Feature importance plot saved\")\n",
    "\n",
    "# Save feature importance to CSV\n",
    "importance_df.to_csv(os.path.join(output_dir, 'feature_importance.csv'), index=False)\n",
    "print(\"Feature importance saved to CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Saving and Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Best model AUC: {comparison_df.iloc[0]['AUC Score']:.3f}\")\n",
    "\n",
    "# Save models\n",
    "import joblib\n",
    "\n",
    "# Save traditional ML models\n",
    "for model_name, model in trained_models.items():\n",
    "    if model_name != 'Deep Neural Network':\n",
    "        model_path = os.path.join(models_dir, f'{model_name.lower().replace(\" \", \"_\")}_model.pkl')\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"Saved {model_name} to {model_path}\")\n",
    "\n",
    "# Save deep learning model\n",
    "if 'Deep Neural Network' in trained_models:\n",
    "    dnn_path = os.path.join(models_dir, 'deep_neural_network_model.h5')\n",
    "    trained_models['Deep Neural Network'].save(dnn_path)\n",
    "    print(f\"Saved Deep Neural Network to {dnn_path}\")\n",
    "\n",
    "# Save preprocessing objects\n",
    "scaler_path = os.path.join(models_dir, 'feature_scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Saved feature scaler to {scaler_path}\")\n",
    "\n",
    "# Save feature selection information\n",
    "feature_info = {\n",
    "    'selected_features': significant_features if len(significant_features) > 0 else top_features,\n",
    "    'feature_names': feature_names,\n",
    "    'atlas_labels': atlas_labels\n",
    "}\n",
    "\n",
    "import pickle\n",
    "feature_info_path = os.path.join(models_dir, 'feature_info.pkl')\n",
    "with open(feature_info_path, 'wb') as f:\n",
    "    pickle.dump(feature_info, f)\n",
    "print(f\"Saved feature information to {feature_info_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload results to S3\n",
    "def upload_results_to_s3(local_dir, s3_bucket, s3_prefix):\n",
    "    \"\"\"\n",
    "    Upload analysis results to S3\n",
    "    \n",
    "    Parameters:\n",
    "    local_dir (str): Local directory containing results\n",
    "    s3_bucket (str): S3 bucket name\n",
    "    s3_prefix (str): S3 prefix for uploaded files\n",
    "    \"\"\"\n",
    "    uploaded_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, local_dir)\n",
    "            s3_key = f\"{s3_prefix}/results/{relative_path}\"\n",
    "            \n",
    "            try:\n",
    "                s3_client.upload_file(local_path, s3_bucket, s3_key)\n",
    "                uploaded_files.append(f\"s3://{s3_bucket}/{s3_key}\")\n",
    "                print(f\"Uploaded {relative_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {local_path}: {e}\")\n",
    "    \n",
    "    return uploaded_files\n",
    "\n",
    "# Upload all results\n",
    "print(\"Uploading results to S3...\")\n",
    "uploaded_files = upload_results_to_s3(output_dir, bucket, prefix)\n",
    "uploaded_models = upload_results_to_s3(models_dir, bucket, prefix)\n",
    "\n",
    "print(f\"\\nUploaded {len(uploaded_files)} result files\")\n",
    "print(f\"Uploaded {len(uploaded_models)} model files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "from datetime import datetime\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "# Parkinson's Disease fMRI Detection - Analysis Summary Report\n",
    "\n",
    "## Analysis Details\n",
    "- **Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **AWS Region**: {region}\n",
    "- **S3 Bucket**: {bucket}\n",
    "- **Data Prefix**: {prefix}\n",
    "\n",
    "## Dataset Information\n",
    "- **Total Subjects**: {len(valid_subjects)}\n",
    "- **Controls**: {np.sum(y == 0)}\n",
    "- **Patients**: {np.sum(y == 1)}\n",
    "- **Features Extracted**: {X.shape[1]}\n",
    "- **Selected Features**: {X_selected.shape[1]}\n",
    "\n",
    "## Model Performance Summary\n",
    "\"\"\"\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    summary_report += f\"\"\"\n",
    "### {row['Model']}\n",
    "- **Test Accuracy**: {row['Accuracy']:.3f}\n",
    "- **AUC Score**: {row['AUC Score']:.3f}\n",
    "\"\"\"\n",
    "    if not pd.isna(row['CV Mean']):\n",
    "        summary_report += f\"- **Cross-Validation**: {row['CV Mean']:.3f} ¬± {row['CV Std']:.3f}\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "## Best Performing Model\n",
    "- **Model**: {best_model_name}\n",
    "- **Test Accuracy**: {comparison_df.iloc[0]['Accuracy']:.3f}\n",
    "- **AUC Score**: {comparison_df.iloc[0]['AUC Score']:.3f}\n",
    "\n",
    "## Top 5 Most Important Features\n",
    "\"\"\"\n",
    "\n",
    "for i, (_, row) in enumerate(importance_df.head(5).iterrows()):\n",
    "    summary_report += f\"{i+1}. {row['feature']}: {row['importance']:.4f}\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "## Generated Files\n",
    "### Results\n",
    "- Model comparison: model_comparison.csv\n",
    "- Feature statistics: feature_statistics.csv\n",
    "- Feature importance: feature_importance.csv\n",
    "- ROC curves: roc_curves.png\n",
    "- Confusion matrices: confusion_matrices.png\n",
    "- Feature distributions: feature_distributions.png\n",
    "- Feature importance plot: feature_importance.png\n",
    "\n",
    "### Models\n",
    "- Best model: {best_model_name.lower().replace(' ', '_')}_model.pkl\n",
    "- Feature scaler: feature_scaler.pkl\n",
    "- Feature information: feature_info.pkl\n",
    "\n",
    "## S3 Locations\n",
    "- **Results**: s3://{bucket}/{prefix}/results/\n",
    "- **Models**: s3://{bucket}/{prefix}/results/\n",
    "\n",
    "## Usage Instructions\n",
    "1. Download the best performing model and preprocessing objects from S3\n",
    "2. Load the model using joblib (for traditional ML) or tensorflow (for DNN)\n",
    "3. Apply the same feature extraction pipeline to new fMRI data\n",
    "4. Use the saved scaler to normalize features\n",
    "5. Make predictions using the trained model\n",
    "\n",
    "## Notes\n",
    "- This analysis used {len(atlas_labels)} brain regions from the Harvard-Oxford atlas\n",
    "- Features included ROI statistics, functional connectivity, and frequency domain measures\n",
    "- Cross-validation was performed to assess model generalization\n",
    "- Class imbalance was addressed using SMOTE when necessary\n",
    "\n",
    "---\n",
    "*Report generated automatically by Parkinson's fMRI Detection Pipeline*\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "report_path = os.path.join(output_dir, 'analysis_summary.md')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "# Upload summary report\n",
    "s3_client.upload_file(report_path, bucket, f\"{prefix}/results/analysis_summary.md\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARKINSON'S DISEASE fMRI DETECTION ANALYSIS COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(summary_report)\n",
    "\n",
    "print(f\"\\nüéØ Best Model: {best_model_name} (AUC: {comparison_df.iloc[0]['AUC Score']:.3f})\")\n",
    "print(f\"üìä Total Features: {X.shape[1]} ‚Üí Selected: {X_selected.shape[1]}\")\n",
    "print(f\"üìÅ Results saved to: s3://{bucket}/{prefix}/results/\")\n",
    "print(f\"üß† Ready for clinical deployment and validation!\")"
   ]
  }
 ]
}